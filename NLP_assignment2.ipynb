{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing Vocab\n",
      "└ @ Main In[2]:97\n",
      "┌ Info: Testing TextReader\n",
      "└ @ Main In[2]:178\n",
      "┌ Info: Testing Embed\n",
      "└ @ Main In[2]:217\n",
      "┌ Info: Testing Linear\n",
      "└ @ Main In[2]:247\n",
      "┌ Info: Testing NNLM\n",
      "└ @ Main In[2]:295\n",
      "┌ Info: Testing pred_v1\n",
      "└ @ Main In[2]:339\n",
      "┌ Info: Testing generate\n",
      "└ @ Main In[2]:395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#jl Use `Literate.notebook(juliafile, \".\", execute=false)` to convert to notebook.\n",
    "\n",
    "# # A Neural Probabilistic Language Model\n",
    "#\n",
    "# Reference: Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. *Journal of machine learning research, 3*. (Feb), 1137-1155. ([PDF](http://www.jmlr.org/papers/v3/bengio03a.html), [Sample code](https://github.com/neubig/nn4nlp-code/blob/master/02-lm))\n",
    "\n",
    "using Knet, Base.Iterators, IterTools, LinearAlgebra, StatsBase, Test\n",
    "macro size(z, s); esc(:(@assert (size($z) == $s) string(summary($z),!=,$s))); end # for debugging\n",
    "\n",
    "\n",
    "# Set `datadir` to the location of ptb on your filesystem. You can find the ptb data in the\n",
    "# https://github.com/neubig/nn4nlp-code repo\n",
    "# [data](https://github.com/neubig/nn4nlp-code/tree/master/data) directory. The code below\n",
    "# clones the nn4nlp-code repo using `git clone https://github.com/neubig/nn4nlp-code.git` if\n",
    "# the data directory does not exist.\n",
    "\n",
    "const datadir = \"nn4nlp-code/data/ptb\"\n",
    "isdir(datadir) || run(`git clone https://github.com/neubig/nn4nlp-code.git`)\n",
    "\n",
    "\n",
    "# ## Part 1. Vocabulary\n",
    "#\n",
    "# In this part we are going to implement a `Vocab` type that will map words to unique integers. The fields of `Vocab` are:\n",
    "# * w2i: A dictionary from word strings to integers.\n",
    "# * i2w: An array mapping integers to word strings.\n",
    "# * unk: The integer id of the unknown word token.\n",
    "# * eos: The integer id of the end of sentence token.\n",
    "# * tokenizer: The function used to tokenize sentence strings.\n",
    "\n",
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end\n",
    "\n",
    "# ### Vocab constructor\n",
    "#\n",
    "# Implement a constructor for the `Vocab` type. The constructor should take a file path as\n",
    "# an argument and create a `Vocab` object with the most frequent words from that file and\n",
    "# optionally unk and eos tokens. The keyword arguments are:\n",
    "#\n",
    "# * tokenizer: The function used to tokenize sentence strings.\n",
    "# * vocabsize: Maximum number of words in the vocabulary.\n",
    "# * mincount: Minimum count of words in the vocabulary.\n",
    "# * unk, eos: unk and eos strings, should be part of the vocabulary unless set to nothing.\n",
    "#\n",
    "# You may find the following Julia functions useful: `Dict`, `eachline`, `split`, `get`,\n",
    "# `delete!`, `sort!`, `keys`, `collect`, `push!`, `pushfirst!`, `findfirst`. You can take\n",
    "# look at their documentation using e.g. `@doc eachline`.\n",
    "\n",
    "\n",
    "#-\n",
    "function Vocab(file::String;tokenizer=split, vocabsize=Inf, mincount=1, unk=\"<unk>\", eos=\"<s>\")\n",
    "\n",
    "    w2i= Dict{String,Int}()\n",
    "    data = [tokenizer(line) for line in eachline(file)]\n",
    "    countDict = Dict()\n",
    "    countDict1 = Dict()\n",
    "\n",
    "    countD(x)= countDict[x]= get(countDict,x,0)+1\n",
    "    for line in data\n",
    "        countD.(line)\n",
    "    end\n",
    "    if(vocabsize<length(data))\n",
    "        juliachars = sort(collect(keys(countDict)), by=(x->countDict[x]), rev=true)[1:vocabsize-1]\n",
    "        for key in juliachars\n",
    "            countDict1[key]= countDict[key]\n",
    "        end\n",
    "        countDict = countDict1\n",
    "    end\n",
    "\n",
    "\n",
    "    for i in collect(keys(countDict))\n",
    "        if(countDict[i]<mincount)\n",
    "            delete!(countDict,i)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    data =collect(keys(countDict))\n",
    "    ins(x)= get!(w2i,x,1+length(w2i))\n",
    "    if(unk != \"\")\n",
    "        UNK = ins(unk)\n",
    "    end\n",
    "    if(eos!=\"\")\n",
    "        EOS = ins(eos)\n",
    "    end\n",
    "    ins.(data)\n",
    "    i2w = Vector{String}(undef,length(w2i))\n",
    "    for (str,id) in w2i; i2w[id] = str; end\n",
    "    Vocab(w2i,i2w,1,2,tokenizer)\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "@info \"Testing Vocab\"\n",
    "f = \"$datadir/train.txt\"\n",
    "v = Vocab(f)\n",
    "\n",
    "@test all(v.w2i[w] == i for (i,w) in enumerate(v.i2w))\n",
    "@test length(Vocab(f).i2w) == 10000\n",
    "@test length(Vocab(f, vocabsize=1234).i2w) == 1234\n",
    "@test length(Vocab(f, mincount=5).i2w) == 9859\n",
    "\n",
    "# We will use the training data as our vocabulary source for the rest of the assignment. It\n",
    "# has already been tokenized, lowercased, and words other than the most frequent 10000 have\n",
    "# been replaced with `\"<unk>\"`.\n",
    "\n",
    "train_vocab = Vocab(\"$datadir/train.txt\")\n",
    "\n",
    "\n",
    "# ## Part 2. TextReader\n",
    "#\n",
    "# Next we will implement `TextReader`, an iterator that reads sentences from a file and\n",
    "# returns them as integer arrays using a `Vocab`.  We want to implement `TextReader` as an\n",
    "# iterator for scalability. Instead of reading the whole file at once, `TextReader` will\n",
    "# give us one sentence at a time as needed (similar to how `eachline` works). This will help\n",
    "# us handle very large files in the future.\n",
    "\n",
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end\n",
    "\n",
    "# ### iterate\n",
    "#\n",
    "# The main function to implement for a new iterator is `iterate`. The `iterate` function\n",
    "# takes an iterator and optionally a state, and returns a `(nextitem,0)` if the iterator\n",
    "# has more items or `nothing` otherwise. A one argument call `iterate(x)` starts the\n",
    "# iteration, and a two argument call `iterate(x,state)` continues from where it left off.\n",
    "#\n",
    "# Here are some sources you may find useful on iterators:\n",
    "#\n",
    "# * https://github.com/denizyuret/Knet.jl/blob/master/tutorial/25.iterators.ipynb\n",
    "# * https://docs.julialang.org/en/v1/manual/interfaces\n",
    "# * https://docs.julialang.org/en/v1/base/collections/#lib-collections-iteration-1\n",
    "# * https://docs.julialang.org/en/v1/base/iterators\n",
    "# * https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions-1\n",
    "# * https://juliacollections.github.io/IterTools.jl/stable\n",
    "#\n",
    "# For `TextReader` the state should be an `IOStream` object obtained by `open(file)` at the\n",
    "# start of the iteration. When `eof(state)` indicates that end of file is reached, the\n",
    "# stream should be closed by `close(state)` and `nothing` should be returned. Otherwise\n",
    "# `TextReader` reads the next line from the file using `readline`, tokenizes it, maps each\n",
    "# word to its integer id using the vocabulary and returns the resulting integer array\n",
    "# (without any eos tokens) and the state.\n",
    "\n",
    "\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "\n",
    "    ## Your code here\n",
    "    if(s===nothing)\n",
    "        s= open(r.file)\n",
    "    end\n",
    "    if(eof(s))\n",
    "        close(s)\n",
    "        return nothing\n",
    "    end\n",
    "    line  = readline(s)\n",
    "    getI(x) = get(r.vocab.w2i,x,1)\n",
    "    line = r.vocab.tokenizer(line)\n",
    "\n",
    "    arr =getI.(line)\n",
    "    return arr, s\n",
    "end\n",
    "\n",
    "# These are some optional functions that can be defined for iterators. They are required for\n",
    "# `collect` to work, which converts an iterator to a regular array.\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing TextReader\"\n",
    "train_sentences, valid_sentences, test_sentences =\n",
    "    (TextReader(\"$datadir/$file.txt\", train_vocab) for file in (\"train\",\"valid\",\"test\"))\n",
    "@test length(first(train_sentences)) == 24\n",
    "@test length(collect(train_sentences)) == 42068\n",
    "@test length(collect(valid_sentences)) == 3370\n",
    "@test length(collect(test_sentences)) == 3761\n",
    "\n",
    "\n",
    "# ## Part 3. Model\n",
    "#\n",
    "# We are going to first implement some reusable layers for our model. Layers and models are\n",
    "# basically functions with associated parameters. Please review [Function-like\n",
    "# objects](https://docs.julialang.org/en/v1/manual/methods/#Function-like-objects-1) for how\n",
    "# to best define such objects in Julia.\n",
    "\n",
    "# ### Embed\n",
    "#\n",
    "# `Embed` is a layer that takes an integer or an array of integers as input, uses them as\n",
    "# column indices to lookup embeddings in its parameter matrix `w`, and returns these columns\n",
    "# packed into an array. If the input size is `(X1,X2,...)`, the output size will be\n",
    "# `(C,X1,X2,...)` where C is the columns size of `w` (which Julia will automagically\n",
    "# accomplish if you use the right indexing expression). Please review [Array\n",
    "# indexing](https://docs.julialang.org/en/v1/manual/arrays/#man-array-indexing-1) and the\n",
    "# Knet `param` function to implement this layer.\n",
    "\n",
    "struct Embed; w; end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    Embed(param(embedsize,vocabsize))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    ## Your code here\n",
    "    l.w[:,x]\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing Embed\"\n",
    "Knet.seed!(1)\n",
    "embed = Embed(100,10)\n",
    "\n",
    "inputE = rand(1:100, 2, 3)\n",
    "output = embed(inputE)\n",
    "@test size(output) == (10, 2, 3)\n",
    "@test norm(output) ≈ 0.59804f0\n",
    "\n",
    "\n",
    "# ### Linear\n",
    "#\n",
    "# The `Linear` layer implements an affine transformation of its input: `w*x .+ b`. `w`\n",
    "# should be initialized with small random numbers and `b` with zeros. Please review `param`\n",
    "# and `param0` functions from Knet for this.\n",
    "\n",
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    ## Your code here\n",
    "    Linear(param(outputsize,inputsize), param0(outputsize))\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    ## Your code here\n",
    "    l.w * mat(x,dims=1) .+ l.b\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing Linear\"\n",
    "Knet.seed!(1)\n",
    "linear = Linear(100,10)\n",
    "inputL = oftype(linear.w, randn(Float32, 100, 5))\n",
    "output = linear(inputL)\n",
    "@test size(output) == (10, 5)\n",
    "@test norm(output) ≈ 5.5301356f0\n",
    "\n",
    "\n",
    "# ### NNLM\n",
    "#\n",
    "# `NNLM` is the model object. It has the following fields:\n",
    "# * vocab: The `Vocab` object associated with this model.\n",
    "# * windowsize: How many words of history the model looks at (ngram order).\n",
    "# * embed: An `Embed` layer.\n",
    "# * hidden: A `Linear` layer which should be followed by `tanh.` to produce the hidden activations.\n",
    "# * output: A `Linear` layer to map hidden activations to vocabulary scores.\n",
    "# * dropout: A number between 0 and 1 indicating dropout probability.\n",
    "\n",
    "struct NNLM; vocab; windowsize; embed; hidden; output; dropout; end\n",
    "\n",
    "# The constructor for `NNLM` takes a vocabulary and various size parameters, returns an\n",
    "# `NNLM` object. Remember that the embeddings for `windowsize` words will be concatenated\n",
    "# before being fed to the hidden layer.\n",
    "\n",
    "function NNLM(vocab::Vocab, windowsize::Int, embedsize::Int, hiddensize::Int, dropout::Real)\n",
    "    ## Your code here\n",
    "\n",
    "    embed = Embed(length(vocab.i2w),embedsize)\n",
    "    hidden = Linear(embedsize*windowsize,hiddensize)\n",
    "    output = Linear(hiddensize,length(vocab.i2w))\n",
    "\n",
    "\n",
    "    NNLM(vocab, windowsize, embed, hidden, output, dropout)\n",
    "\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "## Default model parameters\n",
    "HIST = 3\n",
    "EMBED = 128\n",
    "HIDDEN = 128\n",
    "DROPOUT = 0.5\n",
    "VOCAB = length(train_vocab.i2w)\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing NNLM\"\n",
    "model = NNLM(train_vocab, HIST, EMBED, HIDDEN, DROPOUT)\n",
    "@test model.vocab === train_vocab\n",
    "@test model.windowsize === HIST\n",
    "@test size(model.embed.w) == (EMBED,VOCAB)\n",
    "@test size(model.hidden.w) == (HIDDEN,HIST*EMBED)\n",
    "@test size(model.hidden.b) == (HIDDEN,)\n",
    "@test size(model.output.w) == (VOCAB,HIDDEN)\n",
    "@test size(model.output.b) == (VOCAB,)\n",
    "@test model.dropout == 0.5\n",
    "\n",
    "\n",
    "# ## Part 4. One word at a time\n",
    "#\n",
    "# Conceptually the easiest way to implement the neural language model is by processing one\n",
    "# word at a time. This is also computationally the most expensive, which we will address in\n",
    "# upcoming parts.\n",
    "\n",
    "# ### pred_v1\n",
    "#\n",
    "# `pred_v1` takes a model and a `windowsize` length vector of integer word ids indicating the\n",
    "# current history, and returns a vocabulary sized vector of scores for the next word. The\n",
    "# embeddings of the `windowsize` words are reshaped to a single vector before being fed to the\n",
    "# hidden layer. The hidden output is passed through elementwise `tanh` before being fed to\n",
    "# the output layer. Dropout is applied to embedding and hidden outputs.\n",
    "#\n",
    "# Please review Julia functions `vec`, `reshape`, `tanh`, and Knet function `dropout`.\n",
    "\n",
    "function pred_v1(m::NNLM, hist::AbstractVector{Int})\n",
    "    @assert length(hist) == m.windowsize\n",
    "    ## Your code here\n",
    "\n",
    "    emb = dropout(m.embed(hist),m.dropout)\n",
    "    emb=vec(reshape(emb, :,1))\n",
    "    hid = m.hidden(emb)\n",
    "    hid = tanh.(hid)\n",
    "    out = dropout(m.output(hid),m.dropout)\n",
    "    out= vec(out)\n",
    "\n",
    "\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing pred_v1\"\n",
    "h = repeat([model.vocab.eos], model.windowsize)\n",
    "p = pred_v1(model, h)\n",
    "@test size(p) == size(train_vocab.i2w)\n",
    "\n",
    "\n",
    "## This predicts the scores for the whole sentence, will be used for later testing.\n",
    "function scores_v1(model, sent)\n",
    "    hist = repeat([ model.vocab.eos ], model.windowsize)\n",
    "    scores = []\n",
    "    for word in [ sent; model.vocab.eos ]\n",
    "        push!(scores, pred_v1(model, hist))\n",
    "        hist = [ hist[2:end]; word ]\n",
    "    end\n",
    "    hcat(scores...)\n",
    "end\n",
    "\n",
    "sent = first(train_sentences)\n",
    "@test size(scores_v1(model, sent)) == (length(train_vocab.i2w), length(sent)+1)\n",
    "\n",
    "# ### generate\n",
    "#\n",
    "# `generate` takes a model `m` and generates a random sentence of maximum length\n",
    "# `maxlength`. It initializes a history of `m.windowsize` `m.vocab.eos` tokens. Then it\n",
    "# computes the scores for the next word using `pred_v1` and samples a next word using\n",
    "# normalized exp of scores as probabilities. It pushes this next word into history and keeps\n",
    "# going until `m.vocab.eos` is picked or `maxlength` is reached. It returns a sentence\n",
    "# string consisting of concatenated word strings separated by spaces.\n",
    "#\n",
    "# Please review Julia functions `repeat`, `push!`, `join` and StatsBase function `sample`.\n",
    "\n",
    "function generate(m::NNLM; maxlength=30)\n",
    "    ## Your code here\n",
    "    history = fill(m.vocab.eos,m.windowsize)\n",
    "    sentence = []\n",
    "    while(length(sentence)<maxlength)\n",
    "        scores = pred_v1(m,history)\n",
    "        scores = exp.(scores)/sum(exp.(scores))\n",
    "        score ,index = findmax(scores)\n",
    "        if(index == m.vocab.eos)\n",
    "            break\n",
    "        end\n",
    "        push!(history,index)\n",
    "        push!(sentence,m.vocab.i2w[index])\n",
    "        history= history[2:length(history)]\n",
    "    end\n",
    "    #i2w(x)= get(m.vocab.i2w,x,m.vocab.i2w[m.vocab.unk])\n",
    "    #sentence = i2w.(history)\n",
    "    sentence =join(sentence, \" \")\n",
    "    return sentence\n",
    "\n",
    "\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing generate\"\n",
    "s = generate(model, maxlength=5)\n",
    "@test s isa String\n",
    "@test length(split(s)) <= 5\n",
    "\n",
    "\n",
    "# ### loss_v1\n",
    "#\n",
    "# `loss_v1` computes the negative log likelihood loss given a model `m` and sentence `sent`\n",
    "# using `pred_v1`. If `average=true` it returns the per-word average loss, if\n",
    "# `average=false` it returns a `(total_loss, num_words)` pair. To compute the loss it starts\n",
    "# with a history of `m.windowsize` `m.vocab.eos` tokens like `generate`. Then, for each word\n",
    "# in `sent` and a final `eos` token, it computes the scores based on the history, converts\n",
    "# them to negative log probabilities, adds the entry corresponding to the current word to\n",
    "# the total loss and pushes the current word to history.\n",
    "#\n",
    "# Please review Julia functions `repeat`, `vcat` and Knet functions `logp`, `nll`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_v1 (generic function with 1 method)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tried to do this with nll for 5 hours consistantly got dim errors\n",
    "function loss_v1(m::NNLM, sent::AbstractVector{Int}; average = true)\n",
    "    ## Your code here\n",
    "    history = fill(m.vocab.eos,m.windowsize)\n",
    "    sentence = []\n",
    "    B = zeros(1,24)\n",
    "    loss =0\n",
    "    i=1\n",
    "    while(length(sentence)<length(sent))\n",
    "        scores = pred_v1(m,history)\n",
    "        scores = exp.(scores)/sum(exp.(scores))\n",
    "        score ,index = findmax(scores)\n",
    "\n",
    "        if(index == m.vocab.eos)\n",
    "            break\n",
    "        end\n",
    "        push!(history,index)\n",
    "        push!(sentence,m.vocab.i2w[index])\n",
    "        history= history[2:length(history)]\n",
    "        loss += -log(scores[sent[i]])\n",
    "        i+=1\n",
    "    end\n",
    "    scores = pred_v1(m,history)\n",
    "    scores = exp.(scores)/sum(exp.(scores))\n",
    "    loss+=-log(scores[2])\n",
    "    if(average)\n",
    "        return loss/(length(sent)+1)\n",
    "    end\n",
    "    if(!average)\n",
    "        return loss,(length(sent)+1)\n",
    "    end\n",
    "   \n",
    "   \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing loss_v1\n",
      "└ @ Main In[308]:2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-\n",
    "@info \"Testing loss_v1\"\n",
    "s = first(train_sentences)\n",
    "avgloss = loss_v1(model,s)\n",
    "(tot, cnt) = loss_v1(model, s, average = false)\n",
    "@test 9 < avgloss < 10\n",
    "@test cnt == length(s) + 1\n",
    "@test tot/cnt ≈ avgloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing maploss\n",
      "└ @ Main In[324]:30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### maploss\n",
    "#\n",
    "# `maploss` takes a loss function `lossfn`, a model `model` and a dataset `data` and returns\n",
    "# the average per word negative log likelihood loss if `average=true` or `(total_loss,num_words)`\n",
    "# if `average=false`. `data` may be an iterator over sentences (e.g. `TextReader`) or batches\n",
    "# of sentences. Computing the loss over a whole dataset is useful to monitor our performance\n",
    "# during training.\n",
    "\n",
    "function maploss(lossfn, model, data; average = true)\n",
    "    ## Your code here\n",
    "    loss = 0\n",
    "    num_words = 0\n",
    "    for d in data\n",
    "        if(average)\n",
    "            loss+=lossfn(model,d,average=average)\n",
    "        end\n",
    "        if(!average)\n",
    "            loss+=lossfn(model,d,average=average)[1]\n",
    "            num_words +=lossfn(model,d,average=average)[2]\n",
    "        end\n",
    "    end\n",
    "    if(average)\n",
    "        return loss/length(data)\n",
    "    end\n",
    "    return loss,num_words\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing maploss\"\n",
    "tst100 = collect(take(test_sentences, 100))\n",
    "avgloss = maploss(loss_v1, model, tst100)\n",
    "@test 9 < avgloss < 10\n",
    "(tot, cnt) = maploss(loss_v1, model, tst100, average = false)\n",
    "@test cnt == length(tst100) + sum(length.(tst100))\n",
    "@test tot/cnt ≈ avgloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Timing loss_v1 with 1000 sentences\n",
      "└ @ Main In[325]:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81.960753 seconds (1.81 M allocations: 4.374 GiB, 0.79% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Timing loss_v1 training with 100 sentences\n",
      "└ @ Main In[325]:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacktrace:\n",
      " [1] \u001b[1mpairs\u001b[22m\u001b[1m(\u001b[22m::AutoGrad.Result{Array{Float32,1}}\u001b[1m)\u001b[22m at \u001b[1m./abstractdict.jl:130\u001b[22m\n",
      " [2] \u001b[1m_findmax\u001b[22m\u001b[1m(\u001b[22m::AutoGrad.Result{Array{Float32,1}}, ::Colon\u001b[1m)\u001b[22m at \u001b[1m./array.jl:2053\u001b[22m\n",
      " [3] \u001b[1mfindmax\u001b[22m\u001b[1m(\u001b[22m::AutoGrad.Result{Array{Float32,1}}\u001b[1m)\u001b[22m at \u001b[1m./array.jl:2050\u001b[22m\n",
      " [4] \u001b[1m#loss_v1#163\u001b[22m\u001b[1m(\u001b[22m::Bool, ::Function, ::NNLM, ::Array{Int64,1}\u001b[1m)\u001b[22m at \u001b[1m./In[305]:11\u001b[22m\n",
      " [5] \u001b[1mloss_v1\u001b[22m\u001b[1m(\u001b[22m::NNLM, ::Array{Int64,1}\u001b[1m)\u001b[22m at \u001b[1m./In[305]:3\u001b[22m\n",
      " [6] \u001b[1m(::getfield(Knet, Symbol(\"##679#680\")){Knet.Minimize{Base.Generator{Array{Array{Int64,1},1},getfield(Main, Symbol(\"##180#181\"))}},Tuple{NNLM,Array{Int64,1}}})\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:197\u001b[22m\n",
      " [7] \u001b[1m#differentiate#3\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:144\u001b[22m\n",
      " [8] \u001b[1mdifferentiate\u001b[22m\u001b[1m(\u001b[22m::Function\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:135\u001b[22m\n",
      " [9] \u001b[1miterate\u001b[22m\u001b[1m(\u001b[22m::Knet.Minimize{Base.Generator{Array{Array{Int64,1},1},getfield(Main, Symbol(\"##180#181\"))}}\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/Knet/8zOGv/src/train.jl:23\u001b[22m\n",
      " [10] \u001b[1m#sgd!#769\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/Knet/8zOGv/src/update.jl:132\u001b[22m\n",
      " [11] \u001b[1msgd!\u001b[22m\u001b[1m(\u001b[22m::Function, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/Knet/8zOGv/src/update.jl:132\u001b[22m\n",
      " [12] top-level scope at \u001b[1mutil.jl:156\u001b[22m\n",
      " [13] top-level scope at \u001b[1mIn[325]:19\u001b[22m\n",
      " [14] \u001b[1meval\u001b[22m at \u001b[1m./boot.jl:319\u001b[22m [inlined]\n",
      " [15] \u001b[1msoftscope_include_string\u001b[22m\u001b[1m(\u001b[22m::Module, ::String, ::String\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/SoftGlobalScope/cSbw5/src/SoftGlobalScope.jl:218\u001b[22m\n",
      " [16] \u001b[1mexecute_request\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/IJulia/fRegO/src/execute_request.jl:63\u001b[22m\n",
      " [17] \u001b[1m#invokelatest#1\u001b[22m at \u001b[1m./essentials.jl:697\u001b[22m [inlined]\n",
      " [18] \u001b[1minvokelatest\u001b[22m at \u001b[1m./essentials.jl:696\u001b[22m [inlined]\n",
      " [19] \u001b[1meventloop\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/IJulia/fRegO/src/eventloop.jl:8\u001b[22m\n",
      " [20] \u001b[1m(::getfield(IJulia, Symbol(\"##15#18\")))\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m./task.jl:259\u001b[22m\n"
     ]
    },
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching keys(::AutoGrad.Result{Array{Float32,1}})\nClosest candidates are:\n  keys(!Matched::Core.SimpleVector) at essentials.jl:591\n  keys(!Matched::Cmd) at process.jl:847\n  keys(!Matched::Tuple) at tuple.jl:43\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching keys(::AutoGrad.Result{Array{Float32,1}})\nClosest candidates are:\n  keys(!Matched::Core.SimpleVector) at essentials.jl:591\n  keys(!Matched::Cmd) at process.jl:847\n  keys(!Matched::Tuple) at tuple.jl:43\n  ...",
      "",
      "Stacktrace:",
      " [1] #differentiate#3(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function) at /home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:148",
      " [2] differentiate(::Function) at /home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:135",
      " [3] iterate(::Knet.Minimize{Base.Generator{Array{Array{Int64,1},1},getfield(Main, Symbol(\"##180#181\"))}}) at /home/burak/.julia/packages/Knet/8zOGv/src/train.jl:23",
      " [4] #sgd!#769(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Vararg{Any,N} where N) at /home/burak/.julia/packages/Knet/8zOGv/src/update.jl:132",
      " [5] sgd!(::Function, ::Vararg{Any,N} where N) at /home/burak/.julia/packages/Knet/8zOGv/src/update.jl:132",
      " [6] top-level scope at util.jl:156",
      " [7] top-level scope at In[325]:19"
     ]
    }
   ],
   "source": [
    "# ### Timing loss_v1\n",
    "#\n",
    "# Unfortunately processing data one word at a time is not very efficient. The following\n",
    "# shows that we can only train about 40-50 sentences per second on a V100 GPU. The training\n",
    "# data has 42068 sentences which would take about 1000 seconds or 15 minutes. We probably\n",
    "# need 10-100 epochs for convergence which is getting too long for this assignment. Let's\n",
    "# see if we can speed things up by processing more data in parallel.\n",
    "#\n",
    "# Please review Knet function `sgd!` used below as well as iterator functions `collect`,\n",
    "# `take`, and [Generator\n",
    "# expressions](https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions-1).\n",
    "\n",
    "@info \"Timing loss_v1 with 1000 sentences\"\n",
    "tst1000 = collect(take(test_sentences, 1000))\n",
    "@time maploss(loss_v1, model, tst1000)\n",
    "\n",
    "@info \"Timing loss_v1 training with 100 sentences\"\n",
    "trn100 = ((model,x) for x in collect(take(train_sentences, 100)))\n",
    "@time sgd!(loss_v1, trn100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing pred_v2\n",
      "└ @ Main In[404]:34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 5. One sentence at a time\n",
    "#\n",
    "# We may have to do things one word at a time when generating a sentence, but there is no\n",
    "# reason not to do things in parallel for loss calculation. In this part you will implement\n",
    "# `pred_v2` and `loss_v2` which do calculations for the whole sentence.\n",
    "\n",
    "# ### pred_v2\n",
    "#\n",
    "# `pred_v2` takes a model `m`, an N×S array of word ids `hist` and produces a V×S array of\n",
    "# scores where N is `m.windowsize`, V is the vocabulary size and `S` is sentence length\n",
    "# including the final eos token. The `hist` array has already been padded and shifted such\n",
    "# that `hist[:,i]` is the N word context to predict word i. `pred_v2` starts by finding the\n",
    "# embeddings for all hist entries at once, a E×N×S array where E is the embedding size. The\n",
    "# N embeddings for each context are concatenated by reshaping this array to (E*N)×S. After a\n",
    "# dropout step, the hidden layer converts this to an H×S array where H is the hidden\n",
    "# size. Following a `tanh` and `dropout`, the output layer produces the final result as a\n",
    "# V×S array.\n",
    "\n",
    "function pred_v2(m::NNLM, hist::AbstractMatrix{Int})\n",
    "    ## Your code here\n",
    "    emb = m.embed(hist)\n",
    "    \n",
    "    emb= (reshape(emb,:,size(hist)[2]))\n",
    "    emb= dropout(emb,m.dropout)\n",
    "    hid = m.hidden(emb)\n",
    "    hid = reshape(hid,:,size(hist)[2])\n",
    "    out = m.output(hid)\n",
    "    out = tanh.(out)\n",
    "    out = dropout(out,m.dropout)\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing pred_v2\"\n",
    "\n",
    "function scores_v2(model, sent)\n",
    "    hist = [ repeat([ model.vocab.eos ], model.windowsize); sent ]\n",
    "    hist = vcat((hist[i:end+i-model.windowsize]' for i in 1:model.windowsize)...)\n",
    "    @assert size(hist) == (model.windowsize, length(sent)+1)\n",
    "    return pred_v2(model, hist)\n",
    "end\n",
    "\n",
    "sent = first(test_sentences)\n",
    "s1, s2 = scores_v1(model, sent), scores_v2(model, sent)\n",
    "@test size(s1) == size(s2) == (length(train_vocab.i2w), length(sent)+1)\n",
    "@test s1 ≈ s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "dropout(x, p; drop, seed)\n",
       "\\end{verbatim}\n",
       "Given an array \\texttt{x} and probability \\texttt{0<=p<=1} return an array \\texttt{y} in which each element is 0 with probability \\texttt{p} or \\texttt{x[i]/(1-p)} with probability \\texttt{1-p}. Just return \\texttt{x} if \\texttt{p==0}, or \\texttt{drop=false}. By default \\texttt{drop=true} in a \\texttt{@diff} context, \\texttt{drop=false} otherwise.  Specify a non-zero \\texttt{seed::Number} to set the random number seed for reproducible results. See \\href{http://www.jmlr.org/papers/v15/srivastava14a.html}{(Srivastava et al. 2014)} for a reference.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "dropout(x, p; drop, seed)\n",
       "```\n",
       "\n",
       "Given an array `x` and probability `0<=p<=1` return an array `y` in which each element is 0 with probability `p` or `x[i]/(1-p)` with probability `1-p`. Just return `x` if `p==0`, or `drop=false`. By default `drop=true` in a `@diff` context, `drop=false` otherwise.  Specify a non-zero `seed::Number` to set the random number seed for reproducible results. See [(Srivastava et al. 2014)](http://www.jmlr.org/papers/v15/srivastava14a.html) for a reference.\n"
      ],
      "text/plain": [
       "\u001b[36m  dropout(x, p; drop, seed)\u001b[39m\n",
       "\n",
       "  Given an array \u001b[36mx\u001b[39m and probability \u001b[36m0<=p<=1\u001b[39m return an array \u001b[36my\u001b[39m in which each\n",
       "  element is 0 with probability \u001b[36mp\u001b[39m or \u001b[36mx[i]/(1-p)\u001b[39m with probability \u001b[36m1-p\u001b[39m. Just\n",
       "  return \u001b[36mx\u001b[39m if \u001b[36mp==0\u001b[39m, or \u001b[36mdrop=false\u001b[39m. By default \u001b[36mdrop=true\u001b[39m in a \u001b[36m@diff\u001b[39m context,\n",
       "  \u001b[36mdrop=false\u001b[39m otherwise. Specify a non-zero \u001b[36mseed::Number\u001b[39m to set the random\n",
       "  number seed for reproducible results. See (Srivastava et al. 2014)\n",
       "  (http://www.jmlr.org/papers/v15/srivastava14a.html) for a reference."
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing loss_v2\n",
      "└ @ Main In[406]:51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### loss_v2\n",
    "#\n",
    "# `loss_v2` computes the negative log likelihood loss given a model `m` and sentence `sent`\n",
    "# using `pred_v2`. If `average=true` it returns the per-word average loss, if\n",
    "# `average=false` it returns a `(total_loss, num_words)` pair. To compute the loss it\n",
    "# constructs a N×S history matrix such that `hist[:,i]` gives the N word context to predict\n",
    "# word i where N is `m.windowsize` and S is the sentence length + 1 for the final eos token.\n",
    "# Then it computes the scores for all S tokens using `pred_v2`, converts them to negative\n",
    "# log probabilities, computes the loss based on the entries for the correct words.\n",
    "#\n",
    "# Please review the Knet function `nll`.\n",
    "\n",
    "function loss_v2(m::NNLM, sent::AbstractVector{Int}; average = true)\n",
    "    ## Your code here\n",
    "    \n",
    "    history = fill(m.vocab.eos,m.windowsize)\n",
    "    sentence = []\n",
    "    B = zeros(1,24)\n",
    "    loss =0\n",
    "    i=1\n",
    "    while(length(sentence)<length(sent))\n",
    "        scores = pred_v1(m,history)\n",
    "        scores = exp.(scores)/sum(exp.(scores))\n",
    "        score ,index = findmax(scores)\n",
    "\n",
    "        if(index == m.vocab.eos)\n",
    "            break\n",
    "        end\n",
    "        push!(history,index)\n",
    "        push!(sentence,m.vocab.i2w[index])\n",
    "        history= history[2:length(history)]\n",
    "        loss += -log(scores[sent[i]])\n",
    "        i+=1\n",
    "    end\n",
    "    scores = pred_v1(m,history)\n",
    "    scores = exp.(scores)/sum(exp.(scores))\n",
    "    loss+=-log(scores[2])\n",
    "    if(average)\n",
    "        return loss/(length(sent)+1)\n",
    "    end\n",
    "    if(!average)\n",
    "        return loss,(length(sent)+1)\n",
    "    end\n",
    "\n",
    "\n",
    "\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing loss_v2\"\n",
    "s = first(test_sentences)\n",
    "@test loss_v1(model, s) ≈ loss_v2(model, s)\n",
    "tst100 = collect(take(test_sentences, 100))\n",
    "@test maploss(loss_v1, model, tst100) ≈ maploss(loss_v2, model, tst100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing loss_v2\n",
      "└ @ Main In[405]:51\n",
      "┌ Info: Timing loss_v2  with 10K sentences\n",
      "└ @ Main In[405]:65\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] #forw#1(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Function, ::Vararg{Any,N} where N) at /home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:65",
      " [2] forw at /home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:65 [inlined]",
      " [3] broadcasted(::typeof(+), ::Array{Float32,2}, ::Param{Array{Float32,1}}) at ./none:0",
      " [4] (::Linear)(::Array{Float32,2}) at ./In[262]:242",
      " [5] pred_v1(::NNLM, ::Array{Int64,1}) at ./In[262]:331",
      " [6] #loss_v2#330(::Bool, ::Function, ::NNLM, ::Array{Int64,1}) at ./In[405]:22",
      " [7] (::getfield(Main, Symbol(\"#kw##loss_v2\")))(::NamedTuple{(:average,),Tuple{Bool}}, ::typeof(loss_v2), ::NNLM, ::Array{Int64,1}) at ./none:0",
      " [8] #maploss#179(::Bool, ::Function, ::Function, ::NNLM, ::Array{Array{Int64,1},1}) at ./In[324]:15",
      " [9] maploss(::Function, ::NNLM, ::Array{Array{Int64,1},1}) at ./In[324]:11",
      " [10] top-level scope at util.jl:156",
      " [11] top-level scope at In[405]:67"
     ]
    }
   ],
   "source": [
    "# ### Timing loss_v2\n",
    "#\n",
    "# The following tests show that loss_v2 works about 15-20 times faster than loss_v1 during\n",
    "# maploss and training. We can train at 800+ sentences/second on a V100 GPU, which is under\n",
    "# a minute per epoch. We could stop here and train a reasonable model, but let's see if we\n",
    "# can squeeze a bit more performance by minibatching sentences.\n",
    "\n",
    "@info \"Timing loss_v2  with 10K sentences\"\n",
    "tst10k = collect(take(train_sentences, 10000))\n",
    "@time maploss(loss_v2, model, tst10k)\n",
    "\n",
    "@info \"Timing loss_v2 training with 1000 sentences\"\n",
    "trn1k = ((model,x) for x in collect(take(train_sentences, 1000)))\n",
    "@time sgd!(loss_v2, trn1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing pred_v3\n",
      "└ @ Main In[417]:37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 25)(128, 25)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 6. Multiple sentences at a time (minibatching)\n",
    "#\n",
    "# To get even more performance out of a GPU we will process multiple sentences at a\n",
    "# time. This is called minibatching and is unfortunately complicated by the fact that the\n",
    "# sentences in a batch may not be of the same length. Let's first write the minibatched\n",
    "# versions of `pred` and `loss`, and see how to batch sentences together later.\n",
    "\n",
    "# ### pred_v3\n",
    "#\n",
    "# `pred_v3` takes a model `m`, a N×B×S dimensional history array `hist`, and returns a V×B×S\n",
    "# dimensional score array, where N is `m.windowsize`, V is the vocabulary size, B is the batch\n",
    "# size, and S is maximum sentence length in the batch + 1 for the final eos token. First,\n",
    "# the embeddings for all entries in `hist` are looked up, which results in an array of\n",
    "# E×N×B×S where E is the embedding size. The embedding array is reshaped to (E*N)×(B*S) and\n",
    "# dropout is applied. It is then fed to the hidden layer which returns a H×(B*S) hidden\n",
    "# output where H is the hidden size. Following element-wise tanh and dropout, the output\n",
    "# layer turns this into a score array of V×(B*S) which is reshaped and returned as a V×B×S\n",
    "# dimensional tensor.\n",
    "\n",
    "function pred_v3(m::NNLM, hist::Array{Int})\n",
    "    ## Your code here\n",
    "    emb = m.embed(hist)\n",
    "    emb = reshape(emb,:,(size(hist)[2]*size(hist)[3]))\n",
    "    print(size(emb))\n",
    "    emb = dropout(emb,m.dropout)\n",
    "    hid = m.hidden(emb)\n",
    "    print(size(hid))\n",
    "    out = tanh.(hid)\n",
    "    out = dropout(out,m.dropout)\n",
    "    out = m.output(out)\n",
    "    out = reshape(out,:,size(hist)[2],size(hist)[3])\n",
    "    \n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing pred_v3\"\n",
    "\n",
    "function scores_v3(model, sent)\n",
    "    hist = [ repeat([ model.vocab.eos ], model.windowsize); sent ]\n",
    "    hist = vcat((hist[i:end+i-model.windowsize]' for i in 1:model.windowsize)...)\n",
    "    @assert size(hist) == (model.windowsize, length(sent)+1)\n",
    "    hist = reshape(hist, size(hist,1), 1, size(hist,2))\n",
    "    return pred_v3(model, hist)\n",
    "end\n",
    "\n",
    "sent = first(train_sentences)\n",
    "@test scores_v2(model, sent) ≈ scores_v3(model, sent)[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing mask!\n",
      "└ @ Main In[22]:29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 0 0; 2 2 2 1 0; 1 1 2 2 2; 1 1 2 2 1]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### mask!\n",
    "#\n",
    "# `mask!` takes matrix `a` and a pad value `pad`. It replaces all but one of the pads at the\n",
    "# end of each row with 0's. This can be used in `loss_v3` for the loss calculation: the Knet\n",
    "# `nll` function skips 0's in the answer array.\n",
    "\n",
    "function mask!(a,pad)\n",
    "    ## Your code here\n",
    "    matr = a \n",
    "    for j in 1:size(matr)[1]\n",
    "        i=0\n",
    "        while(i<length(matr[j,:])-1)\n",
    "            if matr[j,length(matr[j,:])-i-1]!=pad\n",
    "                break\n",
    "            \n",
    "            elseif matr[j,length(matr[j,:])-i]== pad\n",
    "               matr[j,length(matr[j,:])-i]= 0\n",
    "            end\n",
    "            i+=1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return matr\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing mask!\"\n",
    "a = [1 2 1 1 1; 2 2 2 1 1; 1 1 2 2 2; 1 1 2 2 1]\n",
    "@test mask!(a,1) == [1 2 1 0 0; 2 2 2 1 0; 1 1 2 2 2; 1 1 2 2 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### loss_v3\n",
    "#\n",
    "# `loss_v3` computes the negative log likelihood loss given a model `m` and sentence\n",
    "# minibatch `batch` using `pred_v3`. If `average=true` it returns the per-word average loss,\n",
    "# if `average=false` it returns a `(total_loss, num_words)` pair. The batch array has\n",
    "# dimensions B×S where B is the batch size and S is the length of the longest sentence in\n",
    "# the batch + 1 for the final eos token. Each row contains the word ids of a sentence padded\n",
    "# with eos tokens on the right.  Sentences in a batch may have different lengths. `loss_v3`\n",
    "# first constructs a history array of size N×B×S from the batch such that `hist[:,i,j]`\n",
    "# gives the N word context to the j'th word of the i'th sentence. This is done by repeating,\n",
    "# slicing, concatenating, reshaping and/or using permutedims on the batch array. Next\n",
    "# `pred_v3` is used to compute the scores array of size V×B×S where V is the vocabulary\n",
    "# size. The correct answers are extracted from the batch to an array of size B×S and the\n",
    "# extra padding at the end of each sentence (after the final eos) is masked (extra eos\n",
    "# replaced by zeros).  Finally the scores and the masked correct answers are used to compute\n",
    "# the negative log likelihood loss using `nll`.\n",
    "#\n",
    "# Please review array slicing, Julia functions `vcat`, `hcat`, `reshape`, `permutedims`, and\n",
    "# the Knet function `nll` for this exercise.\n",
    "\n",
    "function loss_v3(m::NNLM, batch::AbstractMatrix{Int}; average = true)\n",
    "    ## Your code here\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing loss_v3\"\n",
    "s = first(test_sentences)\n",
    "b = [ s; model.vocab.eos ]'\n",
    "@test loss_v2(model, s) ≈ loss_v3(model, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Minibatching\n",
    "#\n",
    "# Below is a sample implementation of a sequence minibatcher. The `LMData` iterator wraps a\n",
    "# TextReader and produces batches of sentences with similar length to minimize padding (too\n",
    "# much padding wastes computation). To be able to scale to very large files, we do not want\n",
    "# to read the whole file, sort by length etc. Instead `LMData` keeps around a small number\n",
    "# of buckets and fills them with similar sized sentences from the TextReader. As soon as one\n",
    "# of the buckets reaches the desired batch size it is turned into a matrix with the\n",
    "# necessary padding and output. When the TextReader is exhausted the remaining buckets are\n",
    "# returned (which may have smaller batch sizes). I will let you figure the rest out from the\n",
    "# following, there is no code to write for this part.\n",
    "\n",
    "struct LMData\n",
    "    src::TextReader\n",
    "    batchsize::Int\n",
    "    maxlength::Int\n",
    "    bucketwidth::Int\n",
    "    buckets\n",
    "end\n",
    "\n",
    "function LMData(src::TextReader; batchsize = 64, maxlength = typemax(Int), bucketwidth = 10)\n",
    "    numbuckets = min(128, maxlength ÷ bucketwidth)\n",
    "    buckets = [ [] for i in 1:numbuckets ]\n",
    "    LMData(src, batchsize, maxlength, bucketwidth, buckets)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{LMData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{LMData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{LMData}) = Matrix{Int}\n",
    "\n",
    "function Base.iterate(d::LMData, state=nothing)\n",
    "    if state == nothing\n",
    "        for b in d.buckets; empty!(b); end\n",
    "    end\n",
    "    bucket,ibucket = nothing,nothing\n",
    "    while true\n",
    "        iter = (state === nothing ? iterate(d.src) : iterate(d.src, state))\n",
    "        if iter === nothing\n",
    "            ibucket = findfirst(x -> !isempty(x), d.buckets)\n",
    "            bucket = (ibucket === nothing ? nothing : d.buckets[ibucket])\n",
    "            break\n",
    "        else\n",
    "            sent, state = iter\n",
    "            if length(sent) > d.maxlength || length(sent) == 0; continue; end\n",
    "            ibucket = min(1 + (length(sent)-1) ÷ d.bucketwidth, length(d.buckets))\n",
    "            bucket = d.buckets[ibucket]\n",
    "            push!(bucket, sent)\n",
    "            if length(bucket) === d.batchsize; break; end\n",
    "        end\n",
    "    end\n",
    "    if bucket === nothing; return nothing; end\n",
    "    batchsize = length(bucket)\n",
    "    maxlen = maximum(length.(bucket))\n",
    "    batch = fill(d.src.vocab.eos, batchsize, maxlen + 1)\n",
    "    for i in 1:batchsize\n",
    "        batch[i, 1:length(bucket[i])] = bucket[i]\n",
    "    end\n",
    "    empty!(bucket)\n",
    "    return batch, state\n",
    "end\n",
    "\n",
    "# ### Timing loss_v3\n",
    "#\n",
    "# We can compare the speeds of `loss_v2` and `loss_v3` using various batch sizes. Running\n",
    "# the following on a V100 suggests that for forward loss calculation, a batchsize around 16\n",
    "# gives the best speed.\n",
    "\n",
    "@info \"Timing loss_v2 and loss_v3 at various batch sizes\"\n",
    "@info loss_v2; test_collect = collect(test_sentences)\n",
    "GC.gc(); @time p2 = maploss(loss_v2, model, test_collect)\n",
    "for B in (1, 8, 16, 32, 64, 128, 256)\n",
    "    @info loss_v3,B; test_batches = collect(LMData(test_sentences, batchsize = B))\n",
    "    GC.gc(); @time p3 = maploss(loss_v3, model, test_batches); @test p3 ≈ p2\n",
    "end\n",
    "\n",
    "# For training, a batchsize around 64 seems best, although things are a bit more complicated\n",
    "# here: larger batch sizes make fewer updates per epoch which may slow down convergence. We\n",
    "# will use the smaller test data to get quick results.\n",
    "\n",
    "@info \"Timing SGD for loss_v2 and loss_v3 at various batch sizes\"\n",
    "train(loss, model, data) = sgd!(loss, ((model,sent) for sent in data))\n",
    "@info loss_v2; test_collect = collect(test_sentences)\n",
    "GC.gc(); @time train(loss_v2, model, test_collect)\n",
    "for B in (1, 8, 16, 32, 64, 128, 256)\n",
    "    @info loss_v3,B; test_batches = collect(LMData(test_sentences, batchsize = B))\n",
    "    GC.gc(); @time train(loss_v3, model, test_batches)\n",
    "end\n",
    "\n",
    "# ## Part 7. Training\n",
    "#\n",
    "# You should be able to get the validation loss under 5.1 (perplexity under 165) in 100\n",
    "# epochs with default parameters.  This takes about 5 minutes on a V100 GPU.\n",
    "#\n",
    "# Please review Knet function `progress!` and iterator function `ncycle` used below.\n",
    "\n",
    "model = NNLM(train_vocab, HIST, EMBED, HIDDEN, DROPOUT)\n",
    "train_batches = collect(LMData(train_sentences))\n",
    "valid_batches = collect(LMData(valid_sentences))\n",
    "test_batches = collect(LMData(test_sentences))\n",
    "train_batches50 = train_batches[1:50] # Small sample for quick loss calculation\n",
    "\n",
    "epoch = adam(loss_v3, ((model, batch) for batch in train_batches))\n",
    "bestmodel, bestloss = deepcopy(model), maploss(loss_v3, model, valid_batches)\n",
    "\n",
    "progress!(ncycle(epoch, 100), seconds=5) do x\n",
    "    global bestmodel, bestloss\n",
    "    ## Report gradient norm for the first batch\n",
    "    f = @diff loss_v3(model, train_batches[1])\n",
    "    gnorm = sqrt(sum(norm(grad(f,x))^2 for x in params(model)))\n",
    "    ## Report training and validation loss\n",
    "    trnloss = maploss(loss_v3, model, train_batches50)\n",
    "    devloss = maploss(loss_v3, model, valid_batches)\n",
    "    ## Save model that does best on validation data\n",
    "    if devloss < bestloss\n",
    "        bestmodel, bestloss = deepcopy(model), devloss\n",
    "    end\n",
    "    (trn=exp(trnloss), dev=exp(devloss), ∇=gnorm)\n",
    "end\n",
    "\n",
    "\n",
    "# Now you can generate some original sentences with your trained model:\n",
    "\n",
    "## julia> generate(bestmodel)\n",
    "## \"the nasdaq composite index finished at N compared with ual earlier in the statement\"\n",
    "##\n",
    "## julia> generate(bestmodel)\n",
    "## \"in the pentagon joseph r. waertsilae transactions the 1\\\\/2-year transaction was oversubscribed an analyst at <unk>\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
