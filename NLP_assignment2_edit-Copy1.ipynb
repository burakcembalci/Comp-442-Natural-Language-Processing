{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing Vocab\n",
      "└ @ Main In[1]:97\n",
      "┌ Info: Testing TextReader\n",
      "└ @ Main In[1]:178\n",
      "┌ Info: Testing Embed\n",
      "└ @ Main In[1]:217\n",
      "┌ Info: Testing Linear\n",
      "└ @ Main In[1]:247\n",
      "┌ Info: Testing NNLM\n",
      "└ @ Main In[1]:295\n",
      "┌ Info: Testing pred_v1\n",
      "└ @ Main In[1]:340\n",
      "┌ Info: Testing generate\n",
      "└ @ Main In[1]:396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#jl Use `Literate.notebook(juliafile, \".\", execute=false)` to convert to notebook.\n",
    "\n",
    "# # A Neural Probabilistic Language Model\n",
    "#\n",
    "# Reference: Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. *Journal of machine learning research, 3*. (Feb), 1137-1155. ([PDF](http://www.jmlr.org/papers/v3/bengio03a.html), [Sample code](https://github.com/neubig/nn4nlp-code/blob/master/02-lm))\n",
    "\n",
    "using Knet, Base.Iterators, IterTools, LinearAlgebra, StatsBase, Test\n",
    "macro size(z, s); esc(:(@assert (size($z) == $s) string(summary($z),!=,$s))); end # for debugging\n",
    "\n",
    "\n",
    "# Set `datadir` to the location of ptb on your filesystem. You can find the ptb data in the\n",
    "# https://github.com/neubig/nn4nlp-code repo\n",
    "# [data](https://github.com/neubig/nn4nlp-code/tree/master/data) directory. The code below\n",
    "# clones the nn4nlp-code repo using `git clone https://github.com/neubig/nn4nlp-code.git` if\n",
    "# the data directory does not exist.\n",
    "\n",
    "const datadir = \"nn4nlp-code/data/ptb\"\n",
    "isdir(datadir) || run(`git clone https://github.com/neubig/nn4nlp-code.git`)\n",
    "\n",
    "\n",
    "# ## Part 1. Vocabulary\n",
    "#\n",
    "# In this part we are going to implement a `Vocab` type that will map words to unique integers. The fields of `Vocab` are:\n",
    "# * w2i: A dictionary from word strings to integers.\n",
    "# * i2w: An array mapping integers to word strings.\n",
    "# * unk: The integer id of the unknown word token.\n",
    "# * eos: The integer id of the end of sentence token.\n",
    "# * tokenizer: The function used to tokenize sentence strings.\n",
    "\n",
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end\n",
    "\n",
    "# ### Vocab constructor\n",
    "#\n",
    "# Implement a constructor for the `Vocab` type. The constructor should take a file path as\n",
    "# an argument and create a `Vocab` object with the most frequent words from that file and\n",
    "# optionally unk and eos tokens. The keyword arguments are:\n",
    "#\n",
    "# * tokenizer: The function used to tokenize sentence strings.\n",
    "# * vocabsize: Maximum number of words in the vocabulary.\n",
    "# * mincount: Minimum count of words in the vocabulary.\n",
    "# * unk, eos: unk and eos strings, should be part of the vocabulary unless set to nothing.\n",
    "#\n",
    "# You may find the following Julia functions useful: `Dict`, `eachline`, `split`, `get`,\n",
    "# `delete!`, `sort!`, `keys`, `collect`, `push!`, `pushfirst!`, `findfirst`. You can take\n",
    "# look at their documentation using e.g. `@doc eachline`.\n",
    "\n",
    "\n",
    "#-\n",
    "function Vocab(file::String;tokenizer=split, vocabsize=Inf, mincount=1, unk=\"<unk>\", eos=\"<s>\")\n",
    "\n",
    "    w2i= Dict{String,Int}()\n",
    "    data = [tokenizer(line) for line in eachline(file)]\n",
    "    countDict = Dict()\n",
    "    countDict1 = Dict()\n",
    "\n",
    "    countD(x)= countDict[x]= get(countDict,x,0)+1\n",
    "    for line in data\n",
    "        countD.(line)\n",
    "    end\n",
    "    if(vocabsize<length(data))\n",
    "        juliachars = sort(collect(keys(countDict)), by=(x->countDict[x]), rev=true)[1:vocabsize-1]\n",
    "        for key in juliachars\n",
    "            countDict1[key]= countDict[key]\n",
    "        end\n",
    "        countDict = countDict1\n",
    "    end\n",
    "\n",
    "\n",
    "    for i in collect(keys(countDict))\n",
    "        if(countDict[i]<mincount)\n",
    "            delete!(countDict,i)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    data =collect(keys(countDict))\n",
    "    ins(x)= get!(w2i,x,1+length(w2i))\n",
    "    if(unk != \"\")\n",
    "        UNK = ins(unk)\n",
    "    end\n",
    "    if(eos!=\"\")\n",
    "        EOS = ins(eos)\n",
    "    end\n",
    "    ins.(data)\n",
    "    i2w = Vector{String}(undef,length(w2i))\n",
    "    for (str,id) in w2i; i2w[id] = str; end\n",
    "    Vocab(w2i,i2w,1,2,tokenizer)\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "@info \"Testing Vocab\"\n",
    "f = \"$datadir/train.txt\"\n",
    "v = Vocab(f)\n",
    "\n",
    "@test all(v.w2i[w] == i for (i,w) in enumerate(v.i2w))\n",
    "@test length(Vocab(f).i2w) == 10000\n",
    "@test length(Vocab(f, vocabsize=1234).i2w) == 1234\n",
    "@test length(Vocab(f, mincount=5).i2w) == 9859\n",
    "\n",
    "# We will use the training data as our vocabulary source for the rest of the assignment. It\n",
    "# has already been tokenized, lowercased, and words other than the most frequent 10000 have\n",
    "# been replaced with `\"<unk>\"`.\n",
    "\n",
    "train_vocab = Vocab(\"$datadir/train.txt\")\n",
    "\n",
    "\n",
    "# ## Part 2. TextReader\n",
    "#\n",
    "# Next we will implement `TextReader`, an iterator that reads sentences from a file and\n",
    "# returns them as integer arrays using a `Vocab`.  We want to implement `TextReader` as an\n",
    "# iterator for scalability. Instead of reading the whole file at once, `TextReader` will\n",
    "# give us one sentence at a time as needed (similar to how `eachline` works). This will help\n",
    "# us handle very large files in the future.\n",
    "\n",
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end\n",
    "\n",
    "# ### iterate\n",
    "#\n",
    "# The main function to implement for a new iterator is `iterate`. The `iterate` function\n",
    "# takes an iterator and optionally a state, and returns a `(nextitem,0)` if the iterator\n",
    "# has more items or `nothing` otherwise. A one argument call `iterate(x)` starts the\n",
    "# iteration, and a two argument call `iterate(x,state)` continues from where it left off.\n",
    "#\n",
    "# Here are some sources you may find useful on iterators:\n",
    "#\n",
    "# * https://github.com/denizyuret/Knet.jl/blob/master/tutorial/25.iterators.ipynb\n",
    "# * https://docs.julialang.org/en/v1/manual/interfaces\n",
    "# * https://docs.julialang.org/en/v1/base/collections/#lib-collections-iteration-1\n",
    "# * https://docs.julialang.org/en/v1/base/iterators\n",
    "# * https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions-1\n",
    "# * https://juliacollections.github.io/IterTools.jl/stable\n",
    "#\n",
    "# For `TextReader` the state should be an `IOStream` object obtained by `open(file)` at the\n",
    "# start of the iteration. When `eof(state)` indicates that end of file is reached, the\n",
    "# stream should be closed by `close(state)` and `nothing` should be returned. Otherwise\n",
    "# `TextReader` reads the next line from the file using `readline`, tokenizes it, maps each\n",
    "# word to its integer id using the vocabulary and returns the resulting integer array\n",
    "# (without any eos tokens) and the state.\n",
    "\n",
    "\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "\n",
    "    ## Your code here\n",
    "    if(s===nothing)\n",
    "        s= open(r.file)\n",
    "    end\n",
    "    if(eof(s))\n",
    "        close(s)\n",
    "        return nothing\n",
    "    end\n",
    "    line  = readline(s)\n",
    "    getI(x) = get(r.vocab.w2i,x,1)\n",
    "    line = r.vocab.tokenizer(line)\n",
    "\n",
    "    arr =getI.(line)\n",
    "    return arr, s\n",
    "end\n",
    "\n",
    "# These are some optional functions that can be defined for iterators. They are required for\n",
    "# `collect` to work, which converts an iterator to a regular array.\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing TextReader\"\n",
    "train_sentences, valid_sentences, test_sentences =\n",
    "    (TextReader(\"$datadir/$file.txt\", train_vocab) for file in (\"train\",\"valid\",\"test\"))\n",
    "@test length(first(train_sentences)) == 24\n",
    "@test length(collect(train_sentences)) == 42068\n",
    "@test length(collect(valid_sentences)) == 3370\n",
    "@test length(collect(test_sentences)) == 3761\n",
    "\n",
    "\n",
    "# ## Part 3. Model\n",
    "#\n",
    "# We are going to first implement some reusable layers for our model. Layers and models are\n",
    "# basically functions with associated parameters. Please review [Function-like\n",
    "# objects](https://docs.julialang.org/en/v1/manual/methods/#Function-like-objects-1) for how\n",
    "# to best define such objects in Julia.\n",
    "\n",
    "# ### Embed\n",
    "#\n",
    "# `Embed` is a layer that takes an integer or an array of integers as input, uses them as\n",
    "# column indices to lookup embeddings in its parameter matrix `w`, and returns these columns\n",
    "# packed into an array. If the input size is `(X1,X2,...)`, the output size will be\n",
    "# `(C,X1,X2,...)` where C is the columns size of `w` (which Julia will automagically\n",
    "# accomplish if you use the right indexing expression). Please review [Array\n",
    "# indexing](https://docs.julialang.org/en/v1/manual/arrays/#man-array-indexing-1) and the\n",
    "# Knet `param` function to implement this layer.\n",
    "\n",
    "struct Embed; w; end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    Embed(param(embedsize,vocabsize))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    ## Your code here\n",
    "    l.w[:,x]\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing Embed\"\n",
    "Knet.seed!(1)\n",
    "embed = Embed(100,10)\n",
    "\n",
    "inputE = rand(1:100, 2, 3)\n",
    "output = embed(inputE)\n",
    "@test size(output) == (10, 2, 3)\n",
    "@test norm(output) ≈ 0.59804f0\n",
    "\n",
    "\n",
    "# ### Linear\n",
    "#\n",
    "# The `Linear` layer implements an affine transformation of its input: `w*x .+ b`. `w`\n",
    "# should be initialized with small random numbers and `b` with zeros. Please review `param`\n",
    "# and `param0` functions from Knet for this.\n",
    "\n",
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    ## Your code here\n",
    "    Linear(param(outputsize,inputsize), param0(outputsize))\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    ## Your code here\n",
    "    l.w * mat(x,dims=1) .+ l.b\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing Linear\"\n",
    "Knet.seed!(1)\n",
    "linear = Linear(100,10)\n",
    "inputL = oftype(linear.w, randn(Float32, 100, 5))\n",
    "output = linear(inputL)\n",
    "@test size(output) == (10, 5)\n",
    "@test norm(output) ≈ 5.5301356f0\n",
    "\n",
    "\n",
    "# ### NNLM\n",
    "#\n",
    "# `NNLM` is the model object. It has the following fields:\n",
    "# * vocab: The `Vocab` object associated with this model.\n",
    "# * windowsize: How many words of history the model looks at (ngram order).\n",
    "# * embed: An `Embed` layer.\n",
    "# * hidden: A `Linear` layer which should be followed by `tanh.` to produce the hidden activations.\n",
    "# * output: A `Linear` layer to map hidden activations to vocabulary scores.\n",
    "# * dropout: A number between 0 and 1 indicating dropout probability.\n",
    "\n",
    "struct NNLM; vocab; windowsize; embed; hidden; output; dropout; end\n",
    "\n",
    "# The constructor for `NNLM` takes a vocabulary and various size parameters, returns an\n",
    "# `NNLM` object. Remember that the embeddings for `windowsize` words will be concatenated\n",
    "# before being fed to the hidden layer.\n",
    "\n",
    "function NNLM(vocab::Vocab, windowsize::Int, embedsize::Int, hiddensize::Int, dropout::Real)\n",
    "    ## Your code here\n",
    "\n",
    "    embed = Embed(length(vocab.i2w),embedsize)\n",
    "    hidden = Linear(embedsize*windowsize,hiddensize)\n",
    "    output = Linear(hiddensize,length(vocab.i2w))\n",
    "\n",
    "\n",
    "    NNLM(vocab, windowsize, embed, hidden, output, dropout)\n",
    "\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "## Default model parameters\n",
    "HIST = 3\n",
    "EMBED = 128\n",
    "HIDDEN = 128\n",
    "DROPOUT = 0.5\n",
    "VOCAB = length(train_vocab.i2w)\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing NNLM\"\n",
    "model = NNLM(train_vocab, HIST, EMBED, HIDDEN, DROPOUT)\n",
    "@test model.vocab === train_vocab\n",
    "@test model.windowsize === HIST\n",
    "@test size(model.embed.w) == (EMBED,VOCAB)\n",
    "@test size(model.hidden.w) == (HIDDEN,HIST*EMBED)\n",
    "@test size(model.hidden.b) == (HIDDEN,)\n",
    "@test size(model.output.w) == (VOCAB,HIDDEN)\n",
    "@test size(model.output.b) == (VOCAB,)\n",
    "@test model.dropout == 0.5\n",
    "\n",
    "\n",
    "# ## Part 4. One word at a time\n",
    "#\n",
    "# Conceptually the easiest way to implement the neural language model is by processing one\n",
    "# word at a time. This is also computationally the most expensive, which we will address in\n",
    "# upcoming parts.\n",
    "\n",
    "# ### pred_v1\n",
    "#\n",
    "# `pred_v1` takes a model and a `windowsize` length vector of integer word ids indicating the\n",
    "# current history, and returns a vocabulary sized vector of scores for the next word. The\n",
    "# embeddings of the `windowsize` words are reshaped to a single vector before being fed to the\n",
    "# hidden layer. The hidden output is passed through elementwise `tanh` before being fed to\n",
    "# the output layer. Dropout is applied to embedding and hidden outputs.\n",
    "#\n",
    "# Please review Julia functions `vec`, `reshape`, `tanh`, and Knet function `dropout`.\n",
    "\n",
    "function pred_v1(m::NNLM, hist::AbstractVector{Int})\n",
    "    @assert length(hist) == m.windowsize\n",
    "    ## Your code here\n",
    "\n",
    "    emb = m.embed(hist)\n",
    "    emb=vec(reshape(emb, :,1))\n",
    "    emb = dropout(emb,m.dropout)  \n",
    "    hid = m.hidden(emb)\n",
    "    hid = tanh.(hid)\n",
    "    out = dropout(m.output(hid),m.dropout)\n",
    "    out= vec(out)\n",
    "\n",
    "\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing pred_v1\"\n",
    "h = repeat([model.vocab.eos], model.windowsize)\n",
    "p = pred_v1(model, h)\n",
    "@test size(p) == size(train_vocab.i2w)\n",
    "\n",
    "\n",
    "## This predicts the scores for the whole sentence, will be used for later testing.\n",
    "function scores_v1(model, sent)\n",
    "    hist = repeat([ model.vocab.eos ], model.windowsize)\n",
    "    scores = []\n",
    "    for word in [ sent; model.vocab.eos ]\n",
    "        push!(scores, pred_v1(model, hist))\n",
    "        hist = [ hist[2:end]; word ]\n",
    "    end\n",
    "    hcat(scores...)\n",
    "end\n",
    "\n",
    "sent = first(train_sentences)\n",
    "@test size(scores_v1(model, sent)) == (length(train_vocab.i2w), length(sent)+1)\n",
    "\n",
    "# ### generate\n",
    "#\n",
    "# `generate` takes a model `m` and generates a random sentence of maximum length\n",
    "# `maxlength`. It initializes a history of `m.windowsize` `m.vocab.eos` tokens. Then it\n",
    "# computes the scores for the next word using `pred_v1` and samples a next word using\n",
    "# normalized exp of scores as probabilities. It pushes this next word into history and keeps\n",
    "# going until `m.vocab.eos` is picked or `maxlength` is reached. It returns a sentence\n",
    "# string consisting of concatenated word strings separated by spaces.\n",
    "#\n",
    "# Please review Julia functions `repeat`, `push!`, `join` and StatsBase function `sample`.\n",
    "\n",
    "function generate(m::NNLM; maxlength=30)\n",
    "    ## Your code here\n",
    "    history = fill(m.vocab.eos,m.windowsize)\n",
    "    sentence = []\n",
    "    while(length(sentence)<maxlength)\n",
    "        scores = pred_v1(m,history)\n",
    "        scores = exp.(scores)/sum(exp.(scores))\n",
    "        score ,index = findmax(scores)\n",
    "        if(index == m.vocab.eos)\n",
    "            break\n",
    "        end\n",
    "        push!(history,index)\n",
    "        push!(sentence,m.vocab.i2w[index])\n",
    "        history= history[2:length(history)]\n",
    "    end\n",
    "    #i2w(x)= get(m.vocab.i2w,x,m.vocab.i2w[m.vocab.unk])\n",
    "    #sentence = i2w.(history)\n",
    "    sentence =join(sentence, \" \")\n",
    "    return sentence\n",
    "\n",
    "\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing generate\"\n",
    "s = generate(model, maxlength=5)\n",
    "@test s isa String\n",
    "@test length(split(s)) <= 5\n",
    "\n",
    "\n",
    "# ### loss_v1\n",
    "#\n",
    "# `loss_v1` computes the negative log likelihood loss given a model `m` and sentence `sent`\n",
    "# using `pred_v1`. If `average=true` it returns the per-word average loss, if\n",
    "# `average=false` it returns a `(total_loss, num_words)` pair. To compute the loss it starts\n",
    "# with a history of `m.windowsize` `m.vocab.eos` tokens like `generate`. Then, for each word\n",
    "# in `sent` and a final `eos` token, it computes the scores based on the history, converts\n",
    "# them to negative log probabilities, adds the entry corresponding to the current word to\n",
    "# the total loss and pushes the current word to history.\n",
    "#\n",
    "# Please review Julia functions `repeat`, `vcat` and Knet functions `logp`, `nll`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_v1 (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tried to do this with nll for 5 hours consistantly got dim errors\n",
    "function loss_v1(m::NNLM, sent::AbstractVector{Int}; average = true)\n",
    "    ## Your code here\n",
    "    history = fill(m.vocab.eos,m.windowsize)\n",
    "    scores =[]\n",
    "    sent= [sent;m.vocab.eos]\n",
    "    for i = 1:length(sent)\n",
    "        for j = 1:m.windowsize\n",
    "            if(i-j>0)\n",
    "                history[j]=sent[i-j]\n",
    "            end\n",
    "        end\n",
    "        scores =vcat(scores,pred_v1(m,vec(history)))\n",
    "    end\n",
    "    scores = reshape(scores,size(m.vocab.i2w)[1],:)\n",
    "    nll(scores,sent,average= average)\n",
    "end\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing loss_v1\n",
      "└ @ Main In[3]:2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-\n",
    "@info \"Testing loss_v1\"\n",
    "s = first(train_sentences)\n",
    "avgloss = loss_v1(model,s)\n",
    "(tot, cnt) = loss_v1(model, s, average = false)\n",
    "@test 9 < avgloss < 10\n",
    "@test cnt == length(s) + 1\n",
    "@test tot/cnt ≈ avgloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maploss (generic function with 1 method)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### maploss\n",
    "#\n",
    "# `maploss` takes a loss function `lossfn`, a model `model` and a dataset `data` and returns\n",
    "# the average per word negative log likelihood loss if `average=true` or `(total_loss,num_words)`\n",
    "# if `average=false`. `data` may be an iterator over sentences (e.g. `TextReader`) or batches\n",
    "# of sentences. Computing the loss over a whole dataset is useful to monitor our performance\n",
    "# during training.\n",
    "\n",
    "function maploss(lossfn, model, data; average = true)\n",
    "    ## Your code here\n",
    "    loss = 0\n",
    "    num_words = 0\n",
    "    for d in data\n",
    "        if(average)\n",
    "            loss+=lossfn(model,d,average=average)\n",
    "        end\n",
    "        if(!average)\n",
    "            loss+=lossfn(model,d,average=average)[1]\n",
    "            num_words +=lossfn(model,d,average=average)[2]\n",
    "        end\n",
    "    end\n",
    "    if(average)\n",
    "        return loss/length(data)\n",
    "    end\n",
    "    return loss,num_words\n",
    "end\n",
    "\n",
    "#-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing maploss\n",
      "└ @ Main In[5]:30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing maploss\"\n",
    "tst100 = collect(take(test_sentences, 100))\n",
    "avgloss = maploss(loss_v1, model, tst100)\n",
    "@test 9 < avgloss < 10\n",
    "(tot, cnt) = maploss(loss_v1, model, tst100, average = false)\n",
    "@test cnt == length(tst100) + sum(length.(tst100))\n",
    "@test tot/cnt ≈ avgloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Timing loss_v1 with 1000 sentences\n",
      "└ @ Main In[5]:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134.517529 seconds (1.14 G allocations: 44.881 GiB, 13.48% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Timing loss_v1 training with 100 sentences\n",
      "└ @ Main In[5]:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47.134350 seconds (115.57 M allocations: 27.009 GiB, 35.55% gc time)\n"
     ]
    }
   ],
   "source": [
    "# ### Timing loss_v1\n",
    "#\n",
    "# Unfortunately processing data one word at a time is not very efficient. The following\n",
    "# shows that we can only train about 40-50 sentences per second on a V100 GPU. The training\n",
    "# data has 42068 sentences which would take about 1000 seconds or 15 minutes. We probably\n",
    "# need 10-100 epochs for convergence which is getting too long for this assignment. Let's\n",
    "# see if we can speed things up by processing more data in parallel.\n",
    "#\n",
    "# Please review Knet function `sgd!` used below as well as iterator functions `collect`,\n",
    "# `take`, and [Generator \n",
    "# expressions](https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions-1).\n",
    "\n",
    "@info \"Timing loss_v1 with 1000 sentences\"\n",
    "tst1000 = collect(take(test_sentences, 1000))\n",
    "@time maploss(loss_v1, model, tst1000)\n",
    "\n",
    "@info \"Timing loss_v1 training with 100 sentences\"\n",
    "trn100 = ((model,x) for x in collect(take(train_sentences, 100)))\n",
    "@time sgd!(loss_v1, trn100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing pred_v2\n",
      "└ @ Main In[9]:36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 5. One sentence at a time\n",
    "#\n",
    "# We may have to do things one word at a time when generating a sentence, but there is no\n",
    "# reason not to do things in parallel for loss calculation. In this part you will implement\n",
    "# `pred_v2` and `loss_v2` which do calculations for the whole sentence.\n",
    "\n",
    "# ### pred_v2\n",
    "#\n",
    "# `pred_v2` takes a model `m`, an N×S array of word ids `hist` and produces a V×S array of\n",
    "# scores where N is `m.windowsize`, V is the vocabulary size and `S` is sentence length\n",
    "# including the final eos token. The `hist` array has already been padded and shifted such\n",
    "# that `hist[:,i]` is the N word context to predict word i. `pred_v2` starts by finding the\n",
    "# embeddings for all hist entries at once, a E×N×S array where E is the embedding size. The\n",
    "# N embeddings for each context are concatenated by reshaping this array to (E*N)×S. After a\n",
    "# dropout step, the hidden layer converts this to an H×S array where H is the hidden\n",
    "# size. Following a `tanh` and `dropout`, the output layer produces the final result as a\n",
    "# V×S array.\n",
    "\n",
    "function pred_v2(m::NNLM, hist::AbstractMatrix{Int})\n",
    "    ## Your code here\n",
    "    emb = m.embed(hist)\n",
    "    \n",
    "    emb= (reshape(emb,:,size(hist)[2]))\n",
    "    emb= dropout(emb,m.dropout)\n",
    "    hid = m.hidden(emb)\n",
    "    hid = reshape(hid,:,size(hist)[2])\n",
    "    out = tanh.(hid)\n",
    "    out = dropout(out,m.dropout)\n",
    "    out = m.output(out)\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing pred_v2\"\n",
    "\n",
    "function scores_v2(model, sent)\n",
    "    hist = [ repeat([ model.vocab.eos ], model.windowsize); sent ]\n",
    "    hist = vcat((hist[i:end+i-model.windowsize]' for i in 1:model.windowsize)...)\n",
    "    @assert size(hist) == (model.windowsize, length(sent)+1)\n",
    "    return pred_v2(model, hist)\n",
    "end\n",
    "\n",
    "sent = first(test_sentences)\n",
    "s1, s2 = scores_v1(model, sent), scores_v2(model, sent)\n",
    "@test size(s1) == size(s2) == (length(train_vocab.i2w), length(sent)+1)\n",
    "@test s1 ≈ s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing loss_v2\n",
      "└ @ Main In[14]:32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### loss_v2\n",
    "#\n",
    "# `loss_v2` computes the negative log likelihood loss given a model `m` and sentence `sent`\n",
    "# using `pred_v2`. If `average=true` it returns the per-word average loss, if\n",
    "# `average=false` it returns a `(total_loss, num_words)` pair. To compute the loss it\n",
    "# constructs a N×S history matrix such that `hist[:,i]` gives the N word context to predict\n",
    "# word i where N is `m.windowsize` and S is the sentence length + 1 for the final eos token.\n",
    "# Then it computes the scores for all S tokens using `pred_v2`, converts them to negative\n",
    "# log probabilities, computes the loss based on the entries for the correct words.\n",
    "#\n",
    "# Please review the Knet function `nll`.\n",
    "\n",
    "function loss_v2(m::NNLM, sent::AbstractVector{Int}; average = true)\n",
    "    ## Your code here\n",
    "    history = fill(m.vocab.eos,m.windowsize,length(sent)+1)\n",
    "    sent=[sent;m.vocab.eos]\n",
    "    for n  = 1:m.windowsize\n",
    "        for i= 1:length(sent)\n",
    "            if(i-n>0)\n",
    "                history[n,i]=sent[i-n]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    scores = pred_v2(m,history)\n",
    "    for  i = 1:length(sent)\n",
    "    end\n",
    "    return loss = nll(scores,sent,average= average)\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing loss_v2\"\n",
    "s = first(test_sentences)\n",
    "@test loss_v1(model, s) ≈ loss_v2(model, s)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "             \n",
    "tst100 = collect(take(test_sentences, 100))\n",
    "@test maploss(loss_v1, model, tst100) ≈ maploss(loss_v2, model, tst100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array{Int64,1}[[9805, 3220, 1539, 3452, 9516, 3922], [4987, 9381, 8875, 7390, 6018, 6048, 1644, 821, 3452, 4271, 5930, 9659, 929, 8875, 8979, 8892, 5539, 1207, 1534, 6932, 5997, 377, 1089, 3220, 9112, 8875, 7644, 9162, 3220, 8766, 5881, 1557, 834, 5826, 8839, 1089, 1712], [2870, 3362, 8413, 2785, 5608, 8875, 4052, 6932, 3600, 7697, 8558, 1953, 9844, 274, 8753, 5570, 1557, 2813, 8875, 6301, 3281, 9112, 4217, 8110, 2539, 111], [8875, 6932, 6048, 6955, 4036, 9927, 8875, 1191, 888, 3108, 8875, 7387, 2539, 6575, 1089, 337, 1476, 7670, 4382, 9678, 5608, 8875, 6932, 3600, 9059, 2010, 2675, 3452, 3601, 8875, 6301, 9194], [1191, 2622, 9594, 6178, 1557, 7434, 2103, 1557, 8875, 6111, 1557, 4950, 8875, 5213, 3108, 274, 8279, 5073, 1191, 8996, 1089, 6048, 274, 8753], [495, 6301, 1089, 4492, 5520, 2322, 8080, 7525, 6429, 111, 9112, 8318, 1, 4636, 8110, 6119], [448, 1191, 888, 8110, 4380, 159, 8964, 1665, 1274, 1601, 8731, 9163, 8658, 2539, 1286, 556, 5761, 482, 3670, 2539, 1747, 4388], [8875, 1, 492, 415, 1091], [8875, 4786, 6646, 1539, 1], [9059, 2010, 8875, 3194, 4382, 2720, 7357, 1557, 3601, 8875, 3619, 9927, 8875, 3108, 1089, 8875, 7390, 6018, 6048, 1644, 5306, 7667, 1, 8084, 1542, 9645, 5027, 1, 5738, 2429], [1, 9056, 1, 6856, 1089, 3194, 5263, 6898, 6153, 3220, 1101, 8856, 1557, 8753, 8875, 6955, 1101, 3452, 607, 7127, 7494], [1782, 8875, 6232, 1101, 9112, 115, 1, 6481, 65, 9594, 2899, 3452, 9117, 3220], [4065, 2742, 4228, 3332, 115, 8349, 1089, 4027, 5559, 1101, 2720, 3117, 9112, 8875, 6646], [4338, 3066, 4342, 2539, 2870, 274, 6297, 415, 8971, 8558, 9518, 6620, 9659, 2084, 9927, 115, 5220, 6007, 4037, 122, 8875, 6048, 6646, 1539, 2614, 87], [5926, 9112, 115, 1, 521, 8875, 8979, 8892, 7171, 9112, 8766, 3098, 9162, 594, 1479, 115, 5777, 1089, 8558, 7585, 5826, 8957, 1, 2103, 115, 6189, 4993, 6932, 6932, 9404, 9927, 8875, 4037, 9112, 1, 3670, 6243], [1, 3670, 3564, 1557, 6932, 6462, 5262, 115, 4426, 3332, 8875, 1191, 888], [5027, 8875, 5689, 1089, 8875, 4037, 6932, 6462, 5262, 4382, 8054], [8875, 8979, 8892, 7171, 9425, 5027, 6932], [8875, 8979, 8080, 7564, 1539, 2462, 9112, 9432, 3069, 2637, 1557, 8875, 1, 9516, 3922, 3600, 5559, 6340, 5950, 6932, 6932], [9112, 5257, 3069, 4231, 8875, 8979, 8080, 4017, 1539, 8875, 1, 8703, 2539, 8875, 7470, 6081, 8875, 6646, 6161, 6932, 4993, 6932, 6932, 115, 1503, 5608, 9516, 3922], [8875, 8979, 6161, 6932, 6932, 9927, 9516, 3922], [5262, 1089, 4380, 8875, 1697, 1089, 4337, 2769, 4382, 8109, 2935, 4011, 4037, 9659, 1670, 1557, 6298, 2539, 9894, 1479, 8875, 9834, 7854, 6932, 9014, 6868, 1089, 8875, 7383, 8279, 3098, 1, 5761], [1222, 5949, 8080, 3763, 4065, 4993, 8597, 9710, 6297, 8508, 6941, 2609, 3535, 5559, 115, 1759, 4334, 9521, 2539, 4380, 6048, 4334, 653], [5027, 6932, 1369, 46, 5633, 8875, 1, 6298, 8875, 1191, 888, 1539, 1, 3670, 9112, 4380, 9398, 6298], [9927, 8875, 1644, 3108, 929, 3958, 929, 4380, 482, 3670, 9700, 1, 3332, 115, 3281, 5306, 8362, 3860, 3108, 5859], [1299, 274, 2675, 4423, 8355, 789, 8558, 9482, 1782, 8875, 6298, 1], [3332, 5219, 8875, 6646, 6297, 7994, 4384, 1479, 5692, 5608, 8220, 2973, 8080, 7215, 6110, 7204, 3447, 1479, 8875, 1553, 3332, 4047, 2846, 3871, 5692], [2539, 6932, 3178, 5608, 8875, 4380, 3670, 7738, 5633, 6298, 5559, 8875, 4380, 5761, 2675, 3452, 1328, 2431, 3332, 8306, 1567], [5027, 5826, 9432, 8875, 8979, 1539, 8610, 1479, 6932, 5997], [8875, 6646, 1], [9710, 2675, 3452, 2948, 8558, 4380, 6048, 4987, 6253, 9819, 5803, 1089, 2153, 259, 460, 6048, 6253, 6297], [3332, 308, 8558, 6301, 4647, 3670, 2262, 1557, 4423, 242, 9112, 4145, 5761, 8537, 9425, 8610, 6932, 6932, 1557, 6932, 6932, 4767, 1552, 7745, 8537, 6161, 6932, 6932, 1557, 6932, 6932, 2539, 1, 3878, 8537, 9630, 6932, 1557, 6932, 6932], [54, 8110, 8565, 2759], [4987, 929, 3281, 2912, 4065, 8865, 1557, 3433, 3683, 8110, 1308, 929, 9163, 8658, 2539, 9158, 8869, 2355, 1557, 3458, 8558, 1437], [1782, 3670, 1539, 3679, 9112, 9163, 8658, 8875, 6048, 1539, 3670, 5027, 6932, 8610, 6932, 6932, 9381, 6691, 9425, 6932, 6932, 9467, 5027, 6932], [6301, 1, 122, 1089, 9543, 1089, 5867, 7659, 1738, 8537, 2742, 8672, 8279, 4404, 1782, 3379, 4271, 1557, 6409, 5566], [377, 1089, 8875, 6048, 6301, 9194, 5633, 5995, 1222, 5949, 8333, 7934, 4033, 6929, 274], [274, 5306, 377, 1089, 8558, 8246, 8874, 722, 9927, 8875, 3312, 4169, 7381, 3372], [5811, 5027, 6932, 8362, 1089, 8875, 6646, 8080, 1011, 6033, 1530, 9867, 929, 8875, 5953, 6932, 111, 3115, 6297, 1534, 6932, 5997, 9776, 1557, 2168, 115, 1, 3308, 9112, 8875, 8979, 7171], [5036, 3098, 533, 7392, 8279, 8875, 1191, 888, 2539, 8875, 8318, 5644, 1644, 3670, 1539, 6448, 3679, 9112, 8318], [5608, 8875, 3670, 7738, 9112, 8875, 5953, 6932, 4227, 9112, 8318, 9543, 1089, 6301, 2884, 1557, 8602, 8110, 5803, 9927, 8875, 1191, 888, 2539, 3194, 2884, 1557, 1, 3379, 8610], [929, 115, 5483, 8875, 4219, 9049, 8875, 111, 2539, 6048, 4765, 1, 5930], [7861, 8875, 1, 1089, 3663, 111, 8875, 3515, 1089, 101, 274, 7196, 8875, 2309, 6048, 6646, 1101, 5227, 4338, 274, 4382, 8338, 1557, 5187, 6048, 3379, 5911, 9927, 8875, 1191, 888], [8875, 111, 7738, 1539, 6481, 1, 8279, 1191, 888, 3108, 274], [3220, 1, 9801, 2103, 5306, 8362, 8246, 6955], [5826, 3664, 2057, 3679, 8362, 5918, 1089, 6929, 3670, 6048, 6429, 8873, 5559, 5940, 8876, 8875, 111, 2539, 6048, 4765, 2539, 492, 7994, 4686, 8279, 2870, 3332, 8875, 6646, 8080, 1191, 1621], [9112, 115, 3663, 8873, 3433, 6929, 274, 4498, 4993, 3433, 1191, 9786, 1089, 8110, 2539, 3458, 8875, 5505, 9112, 111, 1557, 1181, 9112, 115, 2247, 7873], [1782, 8875, 7383, 8007, 5633, 6456, 3220, 1, 259, 5289, 9700, 6297, 3332, 8875, 5765, 5306, 115, 7693, 8438, 5027, 8362, 1089, 8875, 8946, 9882, 4036], [9700, 821, 3452, 6481, 1328, 115, 7896, 1557, 7024, 8875, 9338, 9700, 8947, 1557, 7024], [4987, 8110, 1845, 6314], [8875, 8979, 7171, 4382, 8610, 6932, 5997, 5027, 6932, 1369, 395, 8875, 1, 7738], [5027, 6932, 1369, 5027, 8875, 5689, 1089, 8875, 3771, 3495, 5032, 8875, 1207, 1539, 8610, 6932, 5997], [8361, 4927, 8875, 8875, 5953, 3670, 7738, 5953, 111, 3433, 1738, 8865, 1, 2103, 9381, 8110, 9112, 7390, 6018, 1845, 6314, 8828], [1191, 888, 6856, 4279, 8842, 9006, 5306, 6541, 8875, 3362, 8465, 683, 817, 1], [8689, 5230, 7196, 3220, 8080, 1, 5027, 5826, 9432, 1557, 1328, 588, 115, 9877, 7507, 6429, 8873, 4334, 6382, 3989, 4993, 6903, 9801], [5036, 4833, 1011, 9101, 1191, 888, 9645, 8004, 1, 304, 9006, 1539, 1703, 1557, 1, 929, 8875, 6646, 1539, 6314, 1539, 2380, 9927, 3098, 1, 5768, 2315, 1557, 8875, 3312, 4385, 8875, 5738, 2539, 1644, 3881, 2539, 8875, 1618, 2946, 888], [7419, 1, 4604, 5027, 115, 5536, 1, 5008, 9927, 8875, 3108, 1089, 8875, 1191, 888, 101, 7419, 2675, 9269, 1, 9927, 3379, 2539, 9398, 6048, 1738], [5027, 1479, 6932, 1369, 46, 5953, 111, 4388, 3670, 2539, 3332, 115, 9536, 6858, 8875, 111, 2539, 6048, 4765, 3946, 1557, 2170, 7323, 9112, 2315], [7387, 1509, 9112, 1557, 8875, 111, 4227], [4987, 8875, 1, 1089, 5953, 111, 3433, 1738, 1039, 9927, 8875, 6646, 2539, 8875, 4219, 3189, 8110, 8865, 1557, 104, 2010], [5027, 1479, 6932, 8875, 5953, 6646, 1, 1557, 5190, 4833, 3514, 1089, 6932, 5997, 8610, 2539, 3670, 1539, 2574, 2010], [111, 274, 8753, 8875, 5953, 1539, 1, 5559, 8875, 8979, 2675, 4271, 929, 8607, 929, 6932, 5997], [4927, 5826, 6858, 9970, 722, 8865, 2087, 8558, 2975, 7439, 7144, 4833, 3600, 6297, 1091], [5027, 8526, 5738, 6153, 8537, 1101, 9666, 1557, 7690, 1557, 9970, 722, 2870, 1, 2975, 4982, 5826, 4334, 4423, 8875, 7644, 1], [5559, 8080, 1782, 4750, 5142, 4080, 6856, 1089, 8875, 9473, 5544, 3139, 1089, 4948, 6157, 1530, 1557, 8875, 5275, 1, 9101, 1557, 4687, 5559, 8875, 521, 1539, 2637, 8068], [8689, 6382, 115, 1, 5559, 5826, 129, 7564, 2454, 1101, 4528, 3399, 1, 1479, 1626], [3220, 4334, 4423, 4400, 1, 1557, 4582, 5579, 2720, 1557, 3433, 1557, 570, 3332, 3098, 7710, 1557, 4498, 304, 4080, 9184, 8875, 2975], [5027, 7612, 3927, 5520, 3139, 8875, 6779, 8080, 9743, 4067, 4194, 115, 6298, 9116, 1539, 7129, 1, 7612, 3927, 9428, 9927, 6646, 3308], [8875, 9116, 3694, 5559, 3117, 2742, 6864, 4698, 9049, 8875, 3282, 1699, 2539, 5559, 1089, 4052, 6932, 2539, 5559, 3117, 2742, 5190, 3870, 2622, 3815, 9112, 8875, 6048, 6646], [4231, 2095, 7863, 4126, 9645, 1089, 3120, 9422, 5096, 6153, 5306, 5559, 9659, 8080, 521, 1101, 1741, 1557, 5252, 7323, 7349, 3189, 2032, 122, 3220, 1, 8875, 3447, 1089, 848], [2539, 8689, 7196, 115, 3786, 1089, 3213, 6501, 1, 9927, 6929, 3670], [3220, 8080, 1741, 1557, 1548, 8875, 9877, 924, 7323, 1557, 8875, 1], [929, 8875, 8979, 1207, 1004, 1557, 8306, 7644, 6932, 9404, 9659, 8875, 5953, 4227, 1411, 2574, 5027, 8306, 1, 3670, 3514], [2095, 1, 1089, 6929, 5859, 1, 2622, 5761, 5306, 6932, 5953, 2460, 4382, 3332, 1284, 9927, 8875, 2969, 8875, 9776, 1089, 7854, 6932, 6462, 9112, 6048], [4987, 3117, 4382, 9805, 7387], [9381, 9659, 8080, 9664, 5259, 1862, 2060, 274, 37, 6730, 722, 3220, 8971, 8875, 6646, 9943, 1557, 2884, 6301, 5826, 6634, 274, 5306], [3663, 111, 2460, 9291, 5027, 8607, 9467, 3379, 6730, 2480, 1089, 8875, 6048, 6646, 1184], [5027, 27, 5566, 8110, 2742, 5252, 2103, 1557, 4423, 1, 8279, 6429, 9710, 7670, 1181, 9112, 9317, 8279, 5073, 111, 1782, 111, 3379, 4271, 2539, 3418, 3433, 3495, 8110], [4987, 2792, 109, 5027, 1824, 3802, 8875, 111, 2539, 8110, 6501, 507, 2454], [8875, 1, 9049, 8875, 6048, 2539, 111, 4765, 9659, 6501, 677, 419, 6430, 9877, 1479, 7144, 1222, 5949, 1101, 487, 7129, 3332, 4833, 3600, 2922], [8875, 1191, 888, 8080, 304, 1, 5306, 2133, 1, 6154, 1539, 5371], [4987, 8875, 1644, 6501, 570, 5027, 8875, 6154, 1089, 4011, 3194, 9112, 4011, 8110], [1392, 9700, 5432, 7233, 115, 2969, 570, 5027, 7188, 2922, 9112, 8537, 9700, 7196, 8875, 1, 1831, 4382, 3452, 4140, 7419, 5306], [2844, 9598, 7651, 5340, 2718, 1557, 1, 1191, 4912, 7977, 5950, 6932, 6932], [4987, 3194, 1893, 6084, 5559, 5230, 929, 9112, 8875, 6932, 3600, 8875, 1, 4036, 1191, 2622, 9594, 5559, 4950, 8875, 6646, 8279, 3670, 1191, 8996, 1089, 6048, 1411, 9927, 8875, 1561, 4927, 9659, 8080, 1], [304, 9006, 5306, 3220, 6501, 7233, 4833, 4037, 4993, 5132, 1557, 527, 7670, 1539, 5073, 2539, 6301, 9659], [538, 9663, 3095, 6932, 1001, 9791, 9927, 7148, 804, 2539, 8875, 1, 3220, 8080, 115, 1857, 4149, 9795, 6081, 1962, 492, 7994, 115, 2070], [3220, 8080, 5811, 8875, 4337, 2070, 1089, 6320, 298, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2539, 1087, 7864, 1, 1, 1087, 7864, 6605, 2539, 5695, 6684, 1962, 5535], [5230, 4982, 4536, 6552, 442, 1557, 3547], [4750, 1], [9164, 1089, 800, 6153, 1388, 7003, 1557, 3433, 8306, 8266, 448, 5358, 1557, 7387, 5559, 4382, 3452, 4243], [8875, 7003, 1548, 1557, 115, 3409, 1089, 7101, 8875, 552, 1089, 5114, 8875, 5545, 6909, 492, 8788, 5826, 8957, 929, 4894, 1089, 115, 7627], [8875, 6909, 5306, 115, 4248, 1089, 8875, 7854, 6932, 6462, 9200, 5995, 8875, 9963, 6501, 4423, 1588, 1557, 9139, 8306, 466, 3228, 2539, 3312, 1831, 6326, 5995, 8875, 6096, 1907, 1, 2541], [8399, 8875, 6909, 8669, 3220, 4334, 3433, 8306, 5395, 5043, 1089, 6168, 3139, 1, 122, 1089, 4452, 1505, 796], [115, 9950, 1089, 1511, 722, 5868, 929, 1, 6508, 3139, 5306, 3220, 492, 8634, 115, 7854, 6932, 6462, 7215, 1567, 3332, 377, 1089, 2905, 4130, 2973, 8080, 3926, 2539, 1, 9050], [8875, 7854, 6932, 6462, 1567, 8826, 8875, 2256, 1089, 3098, 979, 7854, 6932, 6462, 9112, 5717, 4591, 9927, 27, 6240, 2161, 1557, 27, 7970, 8875, 1567], [8875, 5761, 1101, 8321, 8279, 3384, 1, 315, 135, 1529, 1089, 1, 2622, 2973, 9112, 1, 2539, 1453, 8016, 1008, 315, 135, 1089, 8875, 3429, 1008, 6165, 2040], [304, 1, 8080, 6909, 96, 9112, 9612, 3926, 2622, 2539, 6790, 1557, 6382, 7854, 6932, 9014, 9112, 3263, 304, 1008, 1101, 115, 7190, 2539, 115, 5669, 8084, 135, 1089, 2905, 4130], [8875, 3263, 2742, 5371, 4987, 6253, 7851, 3399, 3066, 2539, 2867, 6730, 3026, 4423, 2913, 9112, 2905, 4130, 8080, 3282, 2922, 5306, 304, 1008, 9112, 3098, 6080], [4130, 8080, 1897, 1539, 1557, 9366, 2539, 3433]]"
     ]
    }
   ],
   "source": [
    "print(tst100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Timing loss_v2  with 10K sentences\n",
      "└ @ Main In[16]:8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.412008 seconds (2.44 M allocations: 42.118 GiB, 2.01% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Timing loss_v2 training with 1000 sentences\n",
      "└ @ Main In[16]:12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27.831318 seconds (6.48 M allocations: 16.777 GiB, 4.93% gc time)\n"
     ]
    }
   ],
   "source": [
    "# ### Timing loss_v2\n",
    "#\n",
    "# The following tests show that loss_v2 works about 15-20 times faster than loss_v1 during\n",
    "# maploss and training. We can train at 800+ sentences/second on a V100 GPU, which is under\n",
    "# a minute per epoch. We could stop here and train a reasonable model, but let's see if we\n",
    "# can squeeze a bit more performance by minibatching sentences.\n",
    "\n",
    "@info \"Timing loss_v2  with 10K sentences\"\n",
    "tst10k = collect(take(train_sentences, 10000))\n",
    "@time maploss(loss_v2, model, tst10k)\n",
    "\n",
    "@info \"Timing loss_v2 training with 1000 sentences\"\n",
    "trn1k = ((model,x) for x in collect(take(train_sentences, 1000)))\n",
    "@time sgd!(loss_v2, trn1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing pred_v3\n",
      "└ @ Main In[240]:35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Part 6. Multiple sentences at a time (minibatching)\n",
    "#\n",
    "# To get even more performance out of a GPU we will process multiple sentences at a\n",
    "# time. This is called minibatching and is unfortunately complicated by the fact that the\n",
    "# sentences in a batch may not be of the same length. Let's first write the minibatched\n",
    "# versions of `pred` and `loss`, and see how to batch sentences together later.\n",
    "\n",
    "# ### pred_v3\n",
    "#\n",
    "# `pred_v3` takes a model `m`, a N×B×S dimensional history array `hist`, and returns a V×B×S\n",
    "# dimensional score array, where N is `m.windowsize`, V is the vocabulary size, B is the batch\n",
    "# size, and S is maximum sentence length in the batch + 1 for the final eos token. First,\n",
    "# the embeddings for all entries in `hist` are looked up, which results in an array of\n",
    "# E×N×B×S where E is the embedding size. The embedding array is reshaped to (E*N)×(B*S) and\n",
    "# dropout is applied. It is then fed to the hidden layer which returns a H×(B*S) hidden\n",
    "# output where H is the hidden size. Following element-wise tanh and dropout, the output\n",
    "# layer turns this into a score array of V×(B*S) which is reshaped and returned as a V×B×S\n",
    "# dimensional tensor.\n",
    "\n",
    "function pred_v3(m::NNLM, hist::Array{Int})\n",
    "    ## Your code here\n",
    "    emb = m.embed(hist)\n",
    "    emb = reshape(emb,:,(size(hist)[2]*size(hist)[3]))\n",
    "    emb = dropout(emb,m.dropout)\n",
    "    hid = m.hidden(emb)\n",
    "    out = tanh.(hid)\n",
    "    out = dropout(out,m.dropout)\n",
    "    out = m.output(out)\n",
    "    out = reshape(out,:,size(hist)[2],size(hist)[3])\n",
    "    \n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing pred_v3\"\n",
    "\n",
    "function scores_v3(model, sent)\n",
    "    hist = [ repeat([ model.vocab.eos ], model.windowsize); sent ]\n",
    "    hist = vcat((hist[i:end+i-model.windowsize]' for i in 1:model.windowsize)...)\n",
    "    @assert size(hist) == (model.windowsize, length(sent)+1)\n",
    "    hist = reshape(hist, size(hist,1), 1, size(hist,2))\n",
    "    return pred_v3(model, hist)\n",
    "end\n",
    "\n",
    "sent = first(train_sentences)\n",
    "@test scores_v2(model, sent) ≈ scores_v3(model, sent)[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing mask!\n",
      "└ @ Main In[159]:27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### mask!\n",
    "#\n",
    "# `mask!` takes matrix `a` and a pad value `pad`. It replaces all but one of the pads at the\n",
    "# end of each row with 0's. This can be used in `loss_v3` for the loss calculation: the Knet\n",
    "# `nll` function skips 0's in the answer array.\n",
    "\n",
    "function mask!(a,pad)\n",
    "    ## Your code here\n",
    "    matr = a \n",
    "    for j in 1:size(matr)[1]\n",
    "        i=0\n",
    "        while(i<length(matr[j,:])-1)\n",
    "            if matr[j,length(matr[j,:])-i-1]!=pad\n",
    "                break\n",
    "            \n",
    "            elseif matr[j,length(matr[j,:])-i]== pad\n",
    "               matr[j,length(matr[j,:])-i]= 0\n",
    "            end\n",
    "            i+=1\n",
    "        end\n",
    "    end\n",
    "    return matr\n",
    "end\n",
    "\n",
    "#-\n",
    "\n",
    "@info \"Testing mask!\"\n",
    "a = [1 2 1 1 1; 2 2 2 1 1; 1 1 2 2 2; 1 1 2 2 1]\n",
    "@test mask!(a,1) == [1 2 1 0 0; 2 2 2 1 0; 1 1 2 2 2; 1 1 2 2 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_v3 (generic function with 1 method)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### loss_v3\n",
    "#\n",
    "# `loss_v3` computes the negative log likelihood loss given a model `m` and sentence\n",
    "# minibatch `batch` using `pred_v3`. If `average=true` it returns the per-word average loss,\n",
    "# if `average=false` it returns a `(total_loss, num_words)` pair. The batch array has\n",
    "# dimensions B×S where B is the batch size and S is the length of the longest sentence in\n",
    "# the batch + 1 for the final eos token. Each row contains the word ids of a sentence padded\n",
    "# with eos tokens on the right.  Sentences in a batch may have different lengths. `loss_v3`\n",
    "# first constructs a history array of size N×B×S from the batch such that `hist[:,i,j]`\n",
    "# gives the N word context to the j'th word of the i'th sentence. This is done by repeating,\n",
    "# slicing, concatenating, reshaping and/or using permutedims on the batch array. Next\n",
    "# `pred_v3` is used to compute the scores array of size V×B×S where V is the vocabulary\n",
    "# size. The correct answers are extracted from the batch to an array of size B×S and the\n",
    "# extra padding at the end of each sentence (after the final eos) is masked (extra eos\n",
    "# replaced by zeros).  Finally the scores and the masked correct answers are used to compute\n",
    "# the negative log likelihood loss using `nll`.\n",
    "#\n",
    "# Please review array slicing, Julia functions `vcat`, `hcat`, `reshape`, `permutedims`, and\n",
    "# the Knet function `nll` for this exercise.\n",
    "\n",
    "function loss_v3(m::NNLM, batch::AbstractMatrix{Int}; average = true)\n",
    "    ## Your code here\n",
    "    loss = 0\n",
    "    \n",
    "    history = fill(m.vocab.eos,m.windowsize,size(batch)[1],size(batch)[2])\n",
    "    k, i ,j  = size(history)\n",
    "    scorp= fill(0.0,size(batch)[1],size(batch)[2])\n",
    "    #for x = 1:i\n",
    "        #for z= 1:j\n",
    "          #  for n=1:k\n",
    "         #       if (z-n>0)\n",
    "        #            history[n,x,z] = batch[x,z-n]\n",
    "       #         end\n",
    "      #      end\n",
    "     #   end\n",
    "    #end\n",
    "    #scores = pred_v3(m,history)\n",
    "    \n",
    "    #for x = 1:i\n",
    "     #   for z= 1:j\n",
    "      #      scorep[x,z]= scores[batch[x,z],x,z]\n",
    "       # end\n",
    "    #end\n",
    "    print(size(batch)[1])\n",
    "    for i in 1:size(batch)[1]\n",
    "        scores = scores_v3(m,batch[i,:])\n",
    "        print(size(scores))\n",
    "        print(size(batch))\n",
    "        loss += nll(scores[1:length(batch)-1],mask!(batch[i,:],m.vocab.eos),average= average)\n",
    "    end \n",
    "           \n",
    "    return loss/size(batch)[1]\n",
    "    print(loss/length(b))\n",
    "    scores = nll(scorp,mask!(batch,m.vocab.eos),average= average)\n",
    "    \n",
    "    return scores\n",
    "    \n",
    "end\n",
    "\n",
    "#-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_v3 (generic function with 1 method)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function loss_v3(m::NNLM, batch::AbstractMatrix{Int}; average = true)\n",
    "    ## Your code here\n",
    "    \n",
    "    \n",
    "    history = fill(m.vocab.eos,m.windowsize,size(batch)[1],size(batch)[2])\n",
    "    k, i ,j  = size(history)\n",
    "    #scorep = fill(0.0,size(batch)[1],size(batch)[2])\n",
    "    for x = 1:i\n",
    "        for z= 1:j\n",
    "            for n=1:k\n",
    "                if (z-n>0)\n",
    "                    history[n,x,z] = batch[x,z-n]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    scores = pred_v3(m,history)\n",
    "\n",
    "    #for x = 1:i\n",
    "     #   for z= 1:j\n",
    "      #      scorep[x,z]= scores[batch[x,z],x,z]\n",
    "       # end\n",
    "    #end\n",
    "    scores = nll(scores,mask!(batch,m.vocab.eos),average= average)\n",
    "    \n",
    "    return scores\n",
    "    \n",
    "end\n",
    "\n",
    "#-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9805 3220 1539 3452 9516 3922 2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing loss_v3\n",
      "└ @ Main In[257]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing loss_v3\"\n",
    "s = first(test_sentences)\n",
    "b = [ s; model.vocab.eos ]'\n",
    "print(b)\n",
    "@test loss_v2(model, s) ≈ loss_v3(model, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Minibatching\n",
    "#\n",
    "# Below is a sample implementation of a sequence minibatcher. The `LMData` iterator wraps a\n",
    "# TextReader and produces batches of sentences with similar length to minimize padding (too\n",
    "# much padding wastes computation). To be able to scale to very large files, we do not want\n",
    "# to read the whole file, sort by length etc. Instead `LMData` keeps around a small number\n",
    "# of buckets and fills them with similar sized sentences from the TextReader. As soon as one\n",
    "# of the buckets reaches the desired batch size it is turned into a matrix with the\n",
    "# necessary padding and output. When the TextReader is exhausted the remaining buckets are\n",
    "# returned (which may have smaller batch sizes). I will let you figure the rest out from the\n",
    "# following, there is no code to write for this part.\n",
    "\n",
    "struct LMData\n",
    "    src::TextReader\n",
    "    batchsize::Int\n",
    "    maxlength::Int\n",
    "    bucketwidth::Int\n",
    "    buckets\n",
    "end\n",
    "\n",
    "function LMData(src::TextReader; batchsize = 64, maxlength = typemax(Int), bucketwidth = 10)\n",
    "    numbuckets = min(128, maxlength ÷ bucketwidth)\n",
    "    buckets = [ [] for i in 1:numbuckets ]\n",
    "    LMData(src, batchsize, maxlength, bucketwidth, buckets)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{LMData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{LMData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{LMData}) = Matrix{Int}\n",
    "\n",
    "function Base.iterate(d::LMData, state=nothing)\n",
    "    if state == nothing\n",
    "        for b in d.buckets; empty!(b); end\n",
    "    end\n",
    "    bucket,ibucket = nothing,nothing\n",
    "    while true\n",
    "        iter = (state === nothing ? iterate(d.src) : iterate(d.src, state))\n",
    "        if iter === nothing\n",
    "            ibucket = findfirst(x -> !isempty(x), d.buckets)\n",
    "            bucket = (ibucket === nothing ? nothing : d.buckets[ibucket])\n",
    "            break\n",
    "        else\n",
    "            sent, state = iter\n",
    "            if length(sent) > d.maxlength || length(sent) == 0; continue; end\n",
    "            ibucket = min(1 + (length(sent)-1) ÷ d.bucketwidth, length(d.buckets))\n",
    "            bucket = d.buckets[ibucket]\n",
    "            push!(bucket, sent)\n",
    "            if length(bucket) === d.batchsize; break; end\n",
    "        end\n",
    "    end\n",
    "    if bucket === nothing; return nothing; end\n",
    "    batchsize = length(bucket)\n",
    "    maxlen = maximum(length.(bucket))\n",
    "    batch = fill(d.src.vocab.eos, batchsize, maxlen + 1)\n",
    "    for i in 1:batchsize\n",
    "        batch[i, 1:length(bucket[i])] = bucket[i]\n",
    "    end\n",
    "    empty!(bucket)\n",
    "    return batch, state\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Timing loss_v2 and loss_v3 at various batch sizes\n",
      "└ @ Main In[255]:8\n",
      "┌ Info: loss_v2\n",
      "└ @ Main In[255]:9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61.868216 seconds (907.42 k allocations: 15.622 GiB, 2.35% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (loss_v3, 1)\n",
      "└ @ Main In[255]:12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 63.280479 seconds (976.07 k allocations: 15.625 GiB, 2.37% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (loss_v3, 8)\n",
      "└ @ Main In[255]:12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56.184310 seconds (797.16 k allocations: 18.285 GiB, 7.38% gc time)\n",
      "\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1mIn[255]:13\u001b[22m\n",
      "  Expression: p3 ≈ p2\n",
      "   Evaluated: 6.318357f0 ≈ 6.308608f0\n"
     ]
    },
    {
     "ename": "Test.FallbackTestSetException",
     "evalue": "There was an error during testing",
     "output_type": "error",
     "traceback": [
      "There was an error during testing",
      "",
      "Stacktrace:",
      " [1] record(::Test.FallbackTestSet, ::Test.Fail) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.0/Test/src/Test.jl:707",
      " [2] do_test(::Test.Returned, ::Expr) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.0/Test/src/Test.jl:496",
      " [3] macro expansion at ./logging.jl:310 [inlined]",
      " [4] top-level scope at ./In[255]:12"
     ]
    }
   ],
   "source": [
    "# ### Timing loss_v3\n",
    "#\n",
    "# We can compare the speeds of `loss_v2` and `loss_v3` using various batch sizes. Running\n",
    "# the following on a V100 suggests that for forward loss calculation, a batchsize around 16\n",
    "# gives the best speed.\n",
    "\n",
    "\n",
    "@info \"Timing loss_v2 and loss_v3 at various batch sizes\"\n",
    "@info loss_v2; test_collect = collect(test_sentences)\n",
    "GC.gc(); @time p2 = maploss(loss_v2, model, test_collect)\n",
    "for B in (1, 8,16, 32, 64, 128, 256)\n",
    "    @info loss_v3,B; test_batches = collect(LMData(test_sentences, batchsize = B))\n",
    "    GC.gc(); @time p3 = maploss(loss_v3, model, test_batches); @test p3 ≈ p2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 6.194576f0 ≈ 6.308608f0\n",
    " 6.318357f0 ≈ 6.308608f0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475-element Array{Array{Int64,2},1}:\n",
       " [2870 3362 … 2 2; 1191 2622 … 2 2; … ; 8875 8979 … 2 2; 9112 5257 … 2 2]    \n",
       " [495 6301 … 2 2; 1782 8875 … 2 2; … ; 9927 8875 … 5859 2; 1299 274 … 2 2]   \n",
       " [9805 3220 … 2 2; 8875 1 … 2 2; … ; 8875 6646 … 2 2; 54 8110 … 2 2]         \n",
       " [4987 9381 … 2 2; 8875 6932 … 2 2; … ; 7861 8875 … 2 2; 5826 3664 … 2 2]    \n",
       " [1222 5949 … 2 2; 3332 5219 … 2 2; … ; 9112 115 … 2 2; 1782 8875 … 2 2]     \n",
       " [9710 2675 … 2 2; 6301 1 … 5566 2; … ; 8875 111 … 2 2; 9700 821 … 2 2]      \n",
       " [8361 4927 … 2 2; 8689 5230 … 2 2; … ; 5559 8080 … 8068 2; 3220 4334 … 2 2] \n",
       " [8875 8979 … 2 2; 5027 6932 … 2 2; … ; 8689 6382 … 2 2; 2539 8689 … 2 2]    \n",
       " [5027 7612 … 2 2; 8875 9116 … 2 2; … ; 5027 27 … 2 2; 8875 1 … 2 2]         \n",
       " [3220 8080 … 2 2; 3663 111 … 2 2; … ; 304 9006 … 9659 2; 9164 1089 … 4243 2]\n",
       " [3220 1 … 6955 2; 4987 8110 … 2 2; … ; 4130 8080 … 3433 2; 9700 4331 … 2 2] \n",
       " [1392 9700 … 2 2; 538 9663 … 2 2; … ; 8875 3263 … 2 2; 8875 7854 … 2 2]     \n",
       " [8875 1 … 2 2; 8875 9950 … 2 2; … ; 6081 5926 … 9322 2; 9112 9051 … 2 2]    \n",
       " ⋮                                                                           \n",
       " [4987 8300 … 2 2; 9927 9659 … 2 2; … ; 929 8875 … 2 2; 5036 8875 … 1 2]     \n",
       " [9112 7127 … 2 2; 1936 5320 … 2 2; … ; 115 6069 … 1 2; 4987 8875 … 2 2]     \n",
       " [3117 1539 … 2 2; 1 1 … 3700 2; … ; 8875 6894 … 2 2; 9112 6599 … 2 2]       \n",
       " [115 6406 … 2 2; 9112 115 … 2 2; … ; 5041 492 … 2 2; 8293 3375 … 2 2]       \n",
       " [7107 8469 … 6541 2; 8689 4935 … 2 2; 3117 8080 … 2 2; 6253 1747 … 2 2]     \n",
       " [9801 6382 … 2 2; 7861 8213 … 2 2; … ; 8875 7428 … 2 2; 5027 337 … 3111 2]  \n",
       " [9700 1 … 2 2; 9700 4331 … 2547 2]                                          \n",
       " [8875 1852 … 7424 2]                                                        \n",
       " [4425 6858 … 2 2; 5036 3282 … 2 2; … ; 5027 8875 … 1 2; 9112 9051 … 2 2]    \n",
       " [8364 6501 … 2 2; 3332 8875 … 2 2; … ; 9112 3329 … 2 2; 9645 1 … 6894 2]    \n",
       " [685 1 … 1 2]                                                               \n",
       " [27 9932 … 466 2]                                                           "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batches = collect(LMData(test_sentences, batchsize = 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()"
     ]
    },
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 128×10000 Array{Float32,2} at index [Base.Slice(Base.OneTo(128)), [2; 2; 2]\n\n[2; 2; 2870]\n\n[2; 2870; 3362]\n\n[2870; 3362; 8413]\n\n[3362; 8413; 2785]\n\n[8413; 2785; 5608]\n\n[2785; 5608; 8875]\n\n[5608; 8875; 4052]\n\n[8875; 4052; 6932]\n\n[4052; 6932; 3600]\n\n[6932; 3600; 7697]\n\n[3600; 7697; 8558]\n\n[7697; 8558; 1953]\n\n[8558; 1953; 9844]\n\n[1953; 9844; 274]\n\n[9844; 274; 8753]\n\n[274; 8753; 5570]\n\n[8753; 5570; 1557]\n\n[5570; 1557; 2813]\n\n[1557; 2813; 8875]\n\n[2813; 8875; 6301]\n\n[8875; 6301; 3281]\n\n[6301; 3281; 9112]\n\n[3281; 9112; 4217]\n\n[9112; 4217; 8110]\n\n[4217; 8110; 2539]\n\n[8110; 2539; 111]\n\n[2539; 111; 2]\n\n[111; 2; 0]\n\n[2; 0; 0]\n\n[0; 0; 0]\n\n[0; 0; 0]]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 128×10000 Array{Float32,2} at index [Base.Slice(Base.OneTo(128)), [2; 2; 2]\n\n[2; 2; 2870]\n\n[2; 2870; 3362]\n\n[2870; 3362; 8413]\n\n[3362; 8413; 2785]\n\n[8413; 2785; 5608]\n\n[2785; 5608; 8875]\n\n[5608; 8875; 4052]\n\n[8875; 4052; 6932]\n\n[4052; 6932; 3600]\n\n[6932; 3600; 7697]\n\n[3600; 7697; 8558]\n\n[7697; 8558; 1953]\n\n[8558; 1953; 9844]\n\n[1953; 9844; 274]\n\n[9844; 274; 8753]\n\n[274; 8753; 5570]\n\n[8753; 5570; 1557]\n\n[5570; 1557; 2813]\n\n[1557; 2813; 8875]\n\n[2813; 8875; 6301]\n\n[8875; 6301; 3281]\n\n[6301; 3281; 9112]\n\n[3281; 9112; 4217]\n\n[9112; 4217; 8110]\n\n[4217; 8110; 2539]\n\n[8110; 2539; 111]\n\n[2539; 111; 2]\n\n[111; 2; 0]\n\n[2; 0; 0]\n\n[0; 0; 0]\n\n[0; 0; 0]]",
      "",
      "Stacktrace:",
      " [1] throw_boundserror(::Array{Float32,2}, ::Tuple{Base.Slice{Base.OneTo{Int64}},Array{Int64,3}}) at ./abstractarray.jl:484",
      " [2] checkbounds at ./abstractarray.jl:449 [inlined]",
      " [3] _getindex at ./multidimensional.jl:588 [inlined]",
      " [4] getindex(::Array{Float32,2}, ::Function, ::Array{Int64,3}) at ./abstractarray.jl:905",
      " [5] #forw#1(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Param{Array{Float32,2}}, ::Vararg{Any,N} where N) at /home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:66",
      " [6] forw at /home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:65 [inlined]",
      " [7] getindex(::Param{Array{Float32,2}}, ::Function, ::Array{Int64,3}) at ./none:0",
      " [8] (::Embed)(::Array{Int64,3}) at ./In[1]:212",
      " [9] pred_v3(::NNLM, ::Array{Int64,3}) at ./In[167]:22",
      " [10] scores_v3(::NNLM, ::Array{Int64,1}) at ./In[167]:42",
      " [11] #loss_v3#111(::Bool, ::Function, ::NNLM, ::Array{Int64,2}) at ./In[220]:46",
      " [12] (::getfield(Main, Symbol(\"#kw##loss_v3\")))(::NamedTuple{(:average,),Tuple{Bool}}, ::typeof(loss_v3), ::NNLM, ::Array{Int64,2}) at ./none:0",
      " [13] #maploss#55(::Bool, ::Function, ::Function, ::NNLM, ::Array{Array{Int64,2},1}) at ./In[130]:15",
      " [14] maploss(::Function, ::NNLM, ::Array{Array{Int64,2},1}) at ./In[130]:11",
      " [15] top-level scope at In[222]:1"
     ]
    }
   ],
   "source": [
    "maploss(loss_v3, model, test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Timing SGD for loss_v2 and loss_v3 at various batch sizes\n",
      "└ @ Main In[54]:4\n",
      "┌ Info: loss_v2\n",
      "└ @ Main In[54]:6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87.164994 seconds (2.69 M allocations: 62.130 GiB, 4.45% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (loss_v3, 1)\n",
      "└ @ Main In[54]:9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacktrace:\n",
      " [1] \u001b[1msetindex!\u001b[22m\u001b[1m(\u001b[22m::Array{Float64,2}, ::AutoGrad.Result{Float32}, ::Int64, ::Int64\u001b[1m)\u001b[22m at \u001b[1m./array.jl:771\u001b[22m\n",
      " [2] \u001b[1m#loss_v3#35\u001b[22m\u001b[1m(\u001b[22m::Bool, ::Function, ::NNLM, ::Array{Int64,2}\u001b[1m)\u001b[22m at \u001b[1m./In[50]:41\u001b[22m\n",
      " [3] \u001b[1mloss_v3\u001b[22m\u001b[1m(\u001b[22m::NNLM, ::Array{Int64,2}\u001b[1m)\u001b[22m at \u001b[1m./In[50]:25\u001b[22m\n",
      " [4] \u001b[1m(::getfield(Knet, Symbol(\"##679#680\")){Knet.Minimize{Base.Generator{Array{Array{Int64,2},1},getfield(Main, Symbol(\"##36#37\")){NNLM}}},Tuple{NNLM,Array{Int64,2}}})\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:197\u001b[22m\n",
      " [5] \u001b[1m#differentiate#3\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:144\u001b[22m\n",
      " [6] \u001b[1mdifferentiate\u001b[22m at \u001b[1m/home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:135\u001b[22m [inlined]\n",
      " [7] \u001b[1miterate\u001b[22m\u001b[1m(\u001b[22m::Knet.Minimize{Base.Generator{Array{Array{Int64,2},1},getfield(Main, Symbol(\"##36#37\")){NNLM}}}\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/Knet/8zOGv/src/train.jl:23\u001b[22m\n",
      " [8] \u001b[1m#sgd!#769\u001b[22m at \u001b[1m/home/burak/.julia/packages/Knet/8zOGv/src/update.jl:132\u001b[22m [inlined]\n",
      " [9] \u001b[1msgd!\u001b[22m at \u001b[1m/home/burak/.julia/packages/Knet/8zOGv/src/update.jl:132\u001b[22m [inlined]\n",
      " [10] \u001b[1mtrain\u001b[22m\u001b[1m(\u001b[22m::Function, ::NNLM, ::Array{Array{Int64,2},1}\u001b[1m)\u001b[22m at \u001b[1m./In[54]:5\u001b[22m\n",
      " [11] top-level scope at \u001b[1m./logging.jl:312\u001b[22m\n",
      " [12] \u001b[1meval\u001b[22m at \u001b[1m./boot.jl:319\u001b[22m [inlined]\n",
      " [13] \u001b[1msoftscope_include_string\u001b[22m\u001b[1m(\u001b[22m::Module, ::String, ::String\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/SoftGlobalScope/cSbw5/src/SoftGlobalScope.jl:218\u001b[22m\n",
      " [14] \u001b[1mexecute_request\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/IJulia/fRegO/src/execute_request.jl:63\u001b[22m\n",
      " [15] \u001b[1m#invokelatest#1\u001b[22m at \u001b[1m./essentials.jl:697\u001b[22m [inlined]\n",
      " [16] \u001b[1minvokelatest\u001b[22m at \u001b[1m./essentials.jl:696\u001b[22m [inlined]\n",
      " [17] \u001b[1meventloop\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/IJulia/fRegO/src/eventloop.jl:8\u001b[22m\n",
      " [18] \u001b[1m(::getfield(IJulia, Symbol(\"##15#18\")))\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m./task.jl:259\u001b[22m\n"
     ]
    },
    {
     "ename": "MethodError",
     "evalue": "MethodError: Cannot `convert` an object of type AutoGrad.Result{Float32} to an object of type Float64\nClosest candidates are:\n  convert(::Type{T<:Number}, !Matched::T<:Number) where T<:Number at number.jl:6\n  convert(::Type{T<:Number}, !Matched::Number) where T<:Number at number.jl:7\n  convert(::Type{T<:Number}, !Matched::Base.TwicePrecision) where T<:Number at twiceprecision.jl:250\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: Cannot `convert` an object of type AutoGrad.Result{Float32} to an object of type Float64\nClosest candidates are:\n  convert(::Type{T<:Number}, !Matched::T<:Number) where T<:Number at number.jl:6\n  convert(::Type{T<:Number}, !Matched::Number) where T<:Number at number.jl:7\n  convert(::Type{T<:Number}, !Matched::Base.TwicePrecision) where T<:Number at twiceprecision.jl:250\n  ...",
      "",
      "Stacktrace:",
      " [1] #differentiate#3(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function) at /home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:148",
      " [2] differentiate at /home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:135 [inlined]",
      " [3] iterate(::Knet.Minimize{Base.Generator{Array{Array{Int64,2},1},getfield(Main, Symbol(\"##36#37\")){NNLM}}}) at /home/burak/.julia/packages/Knet/8zOGv/src/train.jl:23",
      " [4] #sgd!#769 at /home/burak/.julia/packages/Knet/8zOGv/src/update.jl:132 [inlined]",
      " [5] sgd! at /home/burak/.julia/packages/Knet/8zOGv/src/update.jl:132 [inlined]",
      " [6] train(::Function, ::NNLM, ::Array{Array{Int64,2},1}) at ./In[54]:5",
      " [7] top-level scope at ./logging.jl:312"
     ]
    }
   ],
   "source": [
    "# For training, a batchsize around 64 seems best, although things are a bit more complicated\n",
    "# here: larger batch sizes make fewer updates per epoch which may slow down convergence. We\n",
    "# will use the smaller test data to get quick results.\n",
    "@info \"Timing SGD for loss_v2 and loss_v3 at various batch sizes\"\n",
    "train(loss, model, data) = sgd!(loss, ((model,sent) for sent in data))\n",
    "@info loss_v2; test_collect = collect(test_sentences)\n",
    "GC.gc(); @time train(loss_v2, model, test_collect)\n",
    "for B in (1, 8, 16, 32, 64, 128, 256)\n",
    "    @info loss_v3,B; test_batches = collect(LMData(test_sentences, batchsize = B))\n",
    "    GC.gc(); @time train(loss_v3, model, test_batches)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacktrace:\n",
      " [1] \u001b[1mpairs\u001b[22m\u001b[1m(\u001b[22m::AutoGrad.Result{Array{Float32,1}}\u001b[1m)\u001b[22m at \u001b[1m./abstractdict.jl:130\u001b[22m\n",
      " [2] \u001b[1m_findmax\u001b[22m\u001b[1m(\u001b[22m::AutoGrad.Result{Array{Float32,1}}, ::Colon\u001b[1m)\u001b[22m at \u001b[1m./array.jl:2053\u001b[22m\n",
      " [3] \u001b[1mfindmax\u001b[22m\u001b[1m(\u001b[22m::AutoGrad.Result{Array{Float32,1}}\u001b[1m)\u001b[22m at \u001b[1m./array.jl:2050\u001b[22m\n",
      " [4] \u001b[1m#loss_v3#29\u001b[22m\u001b[1m(\u001b[22m::Bool, ::Function, ::NNLM, ::Array{Int64,2}\u001b[1m)\u001b[22m at \u001b[1m./In[20]:33\u001b[22m\n",
      " [5] \u001b[1mloss_v3\u001b[22m\u001b[1m(\u001b[22m::NNLM, ::Array{Int64,2}\u001b[1m)\u001b[22m at \u001b[1m./In[20]:25\u001b[22m\n",
      " [6] \u001b[1m(::getfield(Knet, Symbol(\"##679#680\")){Knet.Minimize{Base.Generator{Array{Array{Int64,2},1},getfield(Main, Symbol(\"##35#36\"))}},Tuple{NNLM,Array{Int64,2}}})\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:197\u001b[22m\n",
      " [7] \u001b[1m#differentiate#3\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:144\u001b[22m\n",
      " [8] \u001b[1mdifferentiate\u001b[22m\u001b[1m(\u001b[22m::Function\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:135\u001b[22m\n",
      " [9] \u001b[1miterate\u001b[22m\u001b[1m(\u001b[22m::Knet.Minimize{Base.Generator{Array{Array{Int64,2},1},getfield(Main, Symbol(\"##35#36\"))}}\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/Knet/8zOGv/src/train.jl:23\u001b[22m\n",
      " [10] \u001b[1miterate\u001b[22m at \u001b[1m/home/burak/.julia/packages/IterTools/tZQsB/src/IterTools.jl:794\u001b[22m [inlined]\n",
      " [11] \u001b[1miterate\u001b[22m at \u001b[1m/home/burak/.julia/packages/IterTools/tZQsB/src/IterTools.jl:791\u001b[22m [inlined]\n",
      " [12] \u001b[1miterate\u001b[22m\u001b[1m(\u001b[22m::Knet.Progress{IterTools.NCycle{Knet.Minimize{Base.Generator{Array{Array{Int64,2},1},getfield(Main, Symbol(\"##35#36\"))}}}}\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/Knet/8zOGv/src/progress.jl:64\u001b[22m\n",
      " [13] \u001b[1m#progress!#676\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:seconds,),Tuple{Int64}}}, ::Function, ::Function, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/Knet/8zOGv/src/progress.jl:53\u001b[22m\n",
      " [14] \u001b[1m(::getfield(Knet, Symbol(\"#kw##progress!\")))\u001b[22m\u001b[1m(\u001b[22m::NamedTuple{(:seconds,),Tuple{Int64}}, ::typeof(progress!), ::Function, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m at \u001b[1m./none:0\u001b[22m\n",
      " [15] top-level scope at \u001b[1mIn[25]:16\u001b[22m\n",
      " [16] \u001b[1meval\u001b[22m at \u001b[1m./boot.jl:319\u001b[22m [inlined]\n",
      " [17] \u001b[1msoftscope_include_string\u001b[22m\u001b[1m(\u001b[22m::Module, ::String, ::String\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/SoftGlobalScope/cSbw5/src/SoftGlobalScope.jl:218\u001b[22m\n",
      " [18] \u001b[1mexecute_request\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/IJulia/fRegO/src/execute_request.jl:63\u001b[22m\n",
      " [19] \u001b[1m#invokelatest#1\u001b[22m at \u001b[1m./essentials.jl:697\u001b[22m [inlined]\n",
      " [20] \u001b[1minvokelatest\u001b[22m at \u001b[1m./essentials.jl:696\u001b[22m [inlined]\n",
      " [21] \u001b[1meventloop\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m at \u001b[1m/home/burak/.julia/packages/IJulia/fRegO/src/eventloop.jl:8\u001b[22m\n",
      " [22] \u001b[1m(::getfield(IJulia, Symbol(\"##15#18\")))\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m./task.jl:259\u001b[22m\n"
     ]
    },
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching keys(::AutoGrad.Result{Array{Float32,1}})\nClosest candidates are:\n  keys(!Matched::Core.SimpleVector) at essentials.jl:591\n  keys(!Matched::Cmd) at process.jl:847\n  keys(!Matched::Tuple) at tuple.jl:43\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching keys(::AutoGrad.Result{Array{Float32,1}})\nClosest candidates are:\n  keys(!Matched::Core.SimpleVector) at essentials.jl:591\n  keys(!Matched::Cmd) at process.jl:847\n  keys(!Matched::Tuple) at tuple.jl:43\n  ...",
      "",
      "Stacktrace:",
      " [1] #differentiate#3(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function) at /home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:148",
      " [2] differentiate(::Function) at /home/burak/.julia/packages/AutoGrad/FKOf4/src/core.jl:135",
      " [3] iterate(::Knet.Minimize{Base.Generator{Array{Array{Int64,2},1},getfield(Main, Symbol(\"##35#36\"))}}) at /home/burak/.julia/packages/Knet/8zOGv/src/train.jl:23",
      " [4] iterate at /home/burak/.julia/packages/IterTools/tZQsB/src/IterTools.jl:794 [inlined]",
      " [5] iterate at /home/burak/.julia/packages/IterTools/tZQsB/src/IterTools.jl:791 [inlined]",
      " [6] iterate(::Knet.Progress{IterTools.NCycle{Knet.Minimize{Base.Generator{Array{Array{Int64,2},1},getfield(Main, Symbol(\"##35#36\"))}}}}) at /home/burak/.julia/packages/Knet/8zOGv/src/progress.jl:64",
      " [7] #progress!#676(::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:seconds,),Tuple{Int64}}}, ::Function, ::Function, ::Vararg{Any,N} where N) at /home/burak/.julia/packages/Knet/8zOGv/src/progress.jl:53",
      " [8] (::getfield(Knet, Symbol(\"#kw##progress!\")))(::NamedTuple{(:seconds,),Tuple{Int64}}, ::typeof(progress!), ::Function, ::Vararg{Any,N} where N) at ./none:0",
      " [9] top-level scope at In[25]:16"
     ]
    }
   ],
   "source": [
    "# ## Part 7. Training\n",
    "#\n",
    "# You should be able to get the validation loss under 5.1 (perplexity under 165) in 100\n",
    "# epochs with default parameters.  This takes about 5 minutes on a V100 GPU.\n",
    "#\n",
    "# Please review Knet function `progress!` and iterator function `ncycle` used below.\n",
    "\n",
    "model = NNLM(train_vocab, HIST, EMBED, HIDDEN, DROPOUT)\n",
    "train_batches = collect(LMData(train_sentences))\n",
    "valid_batches = collect(LMData(valid_sentences))\n",
    "test_batches = collect(LMData(test_sentences))\n",
    "train_batches50 = train_batches[1:50] # Small sample for quick loss calculation\n",
    "\n",
    "epoch = adam(loss_v3, ((model, batch) for batch in train_batches))\n",
    "bestmodel, bestloss = deepcopy(model), maploss(loss_v3, model, valid_batches)\n",
    "\n",
    "progress!(ncycle(epoch, 100), seconds=5) do x\n",
    "    global bestmodel, bestloss\n",
    "    ## Report gradient norm for the first batch\n",
    "    f = @diff loss_v3(model, train_batches[1])\n",
    "    gnorm = sqrt(sum(norm(grad(f,x))^2 for x in params(model)))\n",
    "    ## Report training and validation loss\n",
    "    trnloss = maploss(loss_v3, model, train_batches50)\n",
    "    devloss = maploss(loss_v3, model, valid_batches)\n",
    "    ## Save model that does best on validation data\n",
    "    if devloss < bestloss\n",
    "        bestmodel, bestloss = deepcopy(model), devloss\n",
    "    end\n",
    "    (trn=exp(trnloss), dev=exp(devloss), ∇=gnorm)\n",
    "end\n",
    "\n",
    "\n",
    "# Now you can generate some original sentences with your trained model:\n",
    "\n",
    "## julia> generate(bestmodel)\n",
    "## \"the nasdaq composite index finished at N compared with ual earlier in the statement\"\n",
    "##\n",
    "## julia> generate(bestmodel)\n",
    "## \"in the pentagon joseph r. waertsilae transactions the 1\\\\/2-year transaction was oversubscribed an analyst at <unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
