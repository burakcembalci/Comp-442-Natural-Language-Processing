{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "**Reference:** Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" In Advances in neural information processing systems, pp. 3104-3112. 2014. ([Paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks), [Sample code](https://github.com/tensorflow/nmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@size (macro with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet, Test, Base.Iterators, IterTools, Random # , LinearAlgebra, StatsBase\n",
    "using AutoGrad: @gcheck  # to check gradients, use with Float64\n",
    "Knet.atype() = KnetArray{Float32}  # determines what Knet.param() uses.\n",
    "macro size(z, s); esc(:(@assert (size($z) == $s) string(summary($z),!=,$s))); end # for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part -1. Types from the last project\n",
    "\n",
    "Please copy the following types and related functions from the last project: `Vocab`,\n",
    "`TextReader`, `Embed`, `Linear`, `mask!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end\n",
    "\n",
    "# ### Vocab constructor\n",
    "#\n",
    "# Implement a constructor for the `Vocab` type. The constructor should take a file path as\n",
    "# an argument and create a `Vocab` object with the most frequent words from that file and\n",
    "# optionally unk and eos tokens. The keyword arguments are:\n",
    "#\n",
    "# * tokenizer: The function used to tokenize sentence strings.\n",
    "# * vocabsize: Maximum number of words in the vocabulary.\n",
    "# * mincount: Minimum count of words in the vocabulary.\n",
    "# * unk, eos: unk and eos strings, should be part of the vocabulary unless set to nothing.\n",
    "#\n",
    "# You may find the following Julia functions useful: `Dict`, `eachline`, `split`, `get`,\n",
    "# `delete!`, `sort!`, `keys`, `collect`, `push!`, `pushfirst!`, `findfirst`. You can take\n",
    "# look at their documentation using e.g. `@doc eachline`.\n",
    "\n",
    "\n",
    "#-\n",
    "function Vocab(file::String;tokenizer=split, vocabsize=Inf, mincount=1, unk=\"<unk>\", eos=\"<s>\")\n",
    "\n",
    "    w2i= Dict{String,Int}()\n",
    "    data = [tokenizer(line) for line in eachline(file)]\n",
    "    countDict = Dict()\n",
    "    countDict1 = Dict()\n",
    "\n",
    "    countD(x)= countDict[x]= get(countDict,x,0)+1\n",
    "    for line in data\n",
    "        countD.(line)\n",
    "    end\n",
    "    if(vocabsize<length(data))\n",
    "        juliachars = sort(collect(keys(countDict)), by=(x->countDict[x]), rev=true)[1:vocabsize-1]\n",
    "        for key in juliachars\n",
    "            countDict1[key]= countDict[key]\n",
    "        end\n",
    "        countDict = countDict1\n",
    "    end\n",
    "\n",
    "\n",
    "    for i in collect(keys(countDict))\n",
    "        if(countDict[i]<mincount)\n",
    "            delete!(countDict,i)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    data =collect(keys(countDict))\n",
    "    ins(x)= get!(w2i,x,1+length(w2i))\n",
    "    if(unk != \"\")\n",
    "        UNK = ins(unk)\n",
    "    end\n",
    "    if(eos!=\"\")\n",
    "        EOS = ins(eos)\n",
    "    end\n",
    "    ins.(data)\n",
    "    i2w = Vector{String}(undef,length(w2i))\n",
    "    for (str,id) in w2i; i2w[id] = str; end\n",
    "    Vocab(w2i,i2w,1,2,tokenizer)\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Reader\n",
    "\n",
    "\n",
    "# ## Part 2. TextReader\n",
    "#\n",
    "# Next we will implement `TextReader`, an iterator that reads sentences from a file and\n",
    "# returns them as integer arrays using a `Vocab`.  We want to implement `TextReader` as an\n",
    "# iterator for scalability. Instead of reading the whole file at once, `TextReader` will\n",
    "# give us one sentence at a time as needed (similar to how `eachline` works). This will help\n",
    "# us handle very large files in the future.\n",
    "\n",
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end\n",
    "\n",
    "# ### iterate\n",
    "#\n",
    "# The main function to implement for a new iterator is `iterate`. The `iterate` function\n",
    "# takes an iterator and optionally a state, and returns a `(nextitem,0)` if the iterator\n",
    "# has more items or `nothing` otherwise. A one argument call `iterate(x)` starts the\n",
    "# iteration, and a two argument call `iterate(x,state)` continues from where it left off.\n",
    "#\n",
    "# Here are some sources you may find useful on iterators:\n",
    "#\n",
    "# * https://github.com/denizyuret/Knet.jl/blob/master/tutorial/25.iterators.ipynb\n",
    "# * https://docs.julialang.org/en/v1/manual/interfaces\n",
    "# * https://docs.julialang.org/en/v1/base/collections/#lib-collections-iteration-1\n",
    "# * https://docs.julialang.org/en/v1/base/iterators\n",
    "# * https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions-1\n",
    "# * https://juliacollections.github.io/IterTools.jl/stable\n",
    "#\n",
    "# For `TextReader` the state should be an `IOStream` object obtained by `open(file)` at the\n",
    "# start of the iteration. When `eof(state)` indicates that end of file is reached, the\n",
    "# stream should be closed by `close(state)` and `nothing` should be returned. Otherwise\n",
    "# `TextReader` reads the next line from the file using `readline`, tokenizes it, maps each\n",
    "# word to its integer id using the vocabulary and returns the resulting integer array\n",
    "# (without any eos tokens) and the state.\n",
    "\n",
    "\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "\n",
    "    ## Your code here\n",
    "    if(s===nothing)\n",
    "        s= open(r.file)\n",
    "    end\n",
    "    if(eof(s))\n",
    "        close(s)\n",
    "        return nothing\n",
    "    end\n",
    "    line  = readline(s)\n",
    "    getI(x) = get(r.vocab.w2i,x,1)\n",
    "    line = r.vocab.tokenizer(line)\n",
    "\n",
    "    arr =getI.(line)\n",
    "    return arr, s\n",
    "end\n",
    "\n",
    "# These are some optional functions that can be defined for iterators. They are required for\n",
    "# `collect` to work, which converts an iterator to a regular array.\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embed\n",
    "\n",
    "struct Embed; w; end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    Embed(param(embedsize,vocabsize))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    ## Your code here\n",
    "    l.w[:,x]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear\n",
    "\n",
    "\n",
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    ## Your code here\n",
    "    Linear(param(outputsize,inputsize), param0(outputsize))\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    ## Your code here\n",
    "    l.w * mat(x,dims=1) .+ l.b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "param(array; atype)\n",
       "param(dims...; init, atype)\n",
       "param0(dims...; atype)\n",
       "\\end{verbatim}\n",
       "The first form returns \\texttt{Param(atype(array))} where \\texttt{atype=identity} is the default.\n",
       "\n",
       "The second form Returns a randomly initialized \\texttt{Param(atype(init(dims...)))}. By default, \\texttt{init} is \\texttt{xavier} and \\texttt{atype} is \\texttt{KnetArray\\{Float32\\}} if \\texttt{gpu() >= 0}, \\texttt{Array\\{Float32\\}} otherwise. \n",
       "\n",
       "The third form \\texttt{param0} is an alias for \\texttt{param(dims...; init=zeros)}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "param(array; atype)\n",
       "param(dims...; init, atype)\n",
       "param0(dims...; atype)\n",
       "```\n",
       "\n",
       "The first form returns `Param(atype(array))` where `atype=identity` is the default.\n",
       "\n",
       "The second form Returns a randomly initialized `Param(atype(init(dims...)))`. By default, `init` is `xavier` and `atype` is `KnetArray{Float32}` if `gpu() >= 0`, `Array{Float32}` otherwise. \n",
       "\n",
       "The third form `param0` is an alias for `param(dims...; init=zeros)`.\n"
      ],
      "text/plain": [
       "\u001b[36m  param(array; atype)\u001b[39m\n",
       "\u001b[36m  param(dims...; init, atype)\u001b[39m\n",
       "\u001b[36m  param0(dims...; atype)\u001b[39m\n",
       "\n",
       "  The first form returns \u001b[36mParam(atype(array))\u001b[39m where \u001b[36matype=identity\u001b[39m is the\n",
       "  default.\n",
       "\n",
       "  The second form Returns a randomly initialized \u001b[36mParam(atype(init(dims...)))\u001b[39m.\n",
       "  By default, \u001b[36minit\u001b[39m is \u001b[36mxavier\u001b[39m and \u001b[36matype\u001b[39m is \u001b[36mKnetArray{Float32}\u001b[39m if \u001b[36mgpu() >= 0\u001b[39m,\n",
       "  \u001b[36mArray{Float32}\u001b[39m otherwise. \n",
       "\n",
       "  The third form \u001b[36mparam0\u001b[39m is an alias for \u001b[36mparam(dims...; init=zeros)\u001b[39m."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mask! (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mask\n",
    "\n",
    "\n",
    "function mask!(a,pad)\n",
    "    ## Your code here\n",
    "    matr = a \n",
    "    for j in 1:size(matr)[1]\n",
    "        i=0\n",
    "        while(i<length(matr[j,:])-1)\n",
    "            if matr[j,length(matr[j,:])-i-1]!=pad\n",
    "                break\n",
    "            \n",
    "            elseif matr[j,length(matr[j,:])-i]== pad\n",
    "               matr[j,length(matr[j,:])-i]= 0\n",
    "            end\n",
    "            i+=1\n",
    "        end\n",
    "    end\n",
    "    return matr\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Load data\n",
    "\n",
    "We will use the Turkish-English pair from the [TED Talks Dataset](https://github.com/neulab/word-embeddings-for-nmt) for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing data\n",
      "└ @ Main In[7]:17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = \"datasets/tr_to_en\"\n",
    "\n",
    "if !isdir(datadir)\n",
    "    download(\"http://www.phontron.com/data/qi18naacl-dataset.tar.gz\", \"qi18naacl-dataset.tar.gz\")\n",
    "    run(`tar xzf qi18naacl-dataset.tar.gz`)\n",
    "end\n",
    "\n",
    "if !isdefined(Main, :tr_vocab)\n",
    "    tr_vocab = Vocab(\"$datadir/tr.train\", mincount=5)\n",
    "    en_vocab = Vocab(\"$datadir/en.train\", mincount=5)\n",
    "    tr_train = TextReader(\"$datadir/tr.train\", tr_vocab)\n",
    "    en_train = TextReader(\"$datadir/en.train\", en_vocab)\n",
    "    tr_dev = TextReader(\"$datadir/tr.dev\", tr_vocab)\n",
    "    en_dev = TextReader(\"$datadir/en.dev\", en_vocab)\n",
    "    tr_test = TextReader(\"$datadir/tr.test\", tr_vocab)\n",
    "    en_test = TextReader(\"$datadir/en.test\", en_vocab)\n",
    "    @info \"Testing data\"\n",
    "    @test length(tr_vocab.i2w) == 38126\n",
    "    @test length(first(tr_test)) == 16\n",
    "    @test length(collect(tr_test)) == 5029\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Minibatching\n",
    "\n",
    "For minibatching we are going to design a new iterator: `MTData`. This iterator is built\n",
    "on top of two TextReaders `src` and `tgt` that produce parallel sentences for source and\n",
    "target languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MTData\n",
    "    src::TextReader        # reader for source language data\n",
    "    tgt::TextReader        # reader for target language data\n",
    "    batchsize::Int         # desired batch size\n",
    "    maxlength::Int         # skip if source sentence above maxlength\n",
    "    batchmajor::Bool       # batch dims (B,T) if batchmajor=false (default) or (T,B) if true.\n",
    "    bucketwidth::Int       # batch sentences with length within bucketwidth of each other\n",
    "    buckets::Vector        # sentences collected in separate arrays called buckets for each length range\n",
    "    batchmaker::Function   # function that turns a bucket into a batch.\n",
    "end\n",
    "\n",
    "function MTData(src::TextReader, tgt::TextReader; batchmaker = arraybatch, batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 10, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    buckets = [ [] for i in 1:numbuckets ] # buckets[i] is an array of sentence pairs with similar length\n",
    "    MTData(src, tgt, batchsize, maxlength, batchmajor, bucketwidth, buckets, batchmaker)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{MTData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{MTData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{MTData}) = NTuple{2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iterate(::MTData)\n",
    "\n",
    "Define the `iterate` function for the `MTData` iterator. `iterate` should return a\n",
    "`(batch, state)` pair or `nothing` if there are no more batches.  The `batch` is a\n",
    "`(x::Matrix{Int},y::Matrix{Int})` pair where `x` is a `(batchsize,srclength)` batch of\n",
    "source language sentences and `y` is a `(batchsize,tgtlength)` batch of parallel target\n",
    "language translations. The `state` is a pair of `(src_state,tgt_state)` which can be used\n",
    "to iterate `d.src` and `d.tgt` to get more sentences.  `iterate(d)` without a second\n",
    "argument should initialize `d` by emptying its buckets and calling `iterate` on the inner\n",
    "iterators `d.src` and `d.tgt` without a state. Please review the documentation on\n",
    "iterators from the last project.\n",
    "\n",
    "To keep similar length sentences together `MTData` uses arrays of similar length sentence\n",
    "pairs called buckets.  Specifically, the `(src_sentence,tgt_sentence)` pairs coming from\n",
    "`src` and `tgt` are pushed into `d.buckets[i]` when the length of the source sentence is\n",
    "in the range `((i-1)*d.bucketwidth+1):(i*d.bucketwidth)`. When one of the buckets reaches\n",
    "`d.batchsize` `d.batchmaker` is called with the full bucket producing a 2-D batch, the\n",
    "bucket is emptied and the batch is returned. If `src` and `tgt` are exhausted the\n",
    "remaining partially full buckets are turned into batches and returned in any order. If the\n",
    "source sentence length is larger than `length(d.buckets)*d.bucketwidth`, the last bucket\n",
    "is used.\n",
    "\n",
    "Sentences above a certain length can be skipped using the `d.maxlength` field, and\n",
    "transposed `x,y` arrays can be produced using the `d.batchmajor` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.iterate(d::MTData, state=nothing)\n",
    "    if state == nothing\n",
    "        for b in d.buckets; empty!(b); end\n",
    "    end\n",
    "    bucket,ibucket = nothing,nothing\n",
    "    state_src,state_tgt = nothing,nothing\n",
    "\n",
    "    while true\n",
    "        if state === nothing\n",
    "            iter_src=iterate(d.src)\n",
    "            iter_tgt=iterate(d.tgt)\n",
    "        else\n",
    "            state_src = state[1]\n",
    "            state_tgt = state[2]\n",
    "            iter_src=iterate(d.src,state_src)\n",
    "            iter_tgt=iterate(d.tgt,state_tgt)\n",
    "        end\n",
    "        if iter_src === nothing\n",
    "            ibucket = findfirst(x -> !isempty(x), d.buckets)\n",
    "            bucket = (ibucket === nothing ? nothing : d.buckets[ibucket])\n",
    "            break\n",
    "        else\n",
    "            sent_src, state_src = iter_src\n",
    "            sent_tgt, state_tgt= iter_tgt\n",
    "            if length(sent_src) > d.maxlength || length(sent_src) == 0; continue; end\n",
    "            ibucket = min(1 + (length(sent_src)-1) ÷ d.bucketwidth, length(d.buckets))\n",
    "            bucket = d.buckets[ibucket]\n",
    "            push!(bucket, (sent_src,sent_tgt))\n",
    "            if length(bucket) === d.batchsize; break; end\n",
    "        end\n",
    "    end\n",
    "    if bucket === nothing; return nothing; end\n",
    "    \n",
    "    batch = d.batchmaker(d,bucket)\n",
    "\n",
    "    empty!(bucket)\n",
    "    return batch, (state_src,state_tgt)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arraybatch\n",
    "\n",
    "Define `arraybatch(d, bucket)` to be used as the default `d.batchmaker`. `arraybatch`\n",
    "takes an `MTData` object and an array of sentence pairs `bucket` and returns a\n",
    "`(x::Matrix{Int},y::Matrix{Int})` pair where `x` is a `(batchsize,srclength)` batch of\n",
    "source language sentences and `y` is a `(batchsize,tgtlength)` batch of parallel target\n",
    "language translations. Note that the sentences in the bucket do not have any `eos` tokens\n",
    "and they may have different lengths. `arraybatch` should copy the source sentences into\n",
    "`x` padding shorter ones on the left with `eos` tokens. It should copy the target\n",
    "sentences into `y` with an `eos` token in the beginning and end of each sentence and\n",
    "shorter sentences padded on the right with extra `eos` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arraybatch (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function arraybatch(d::MTData, bucket)\n",
    "    # Your code here\n",
    "    bucketx = map(x->x[1],bucket)\n",
    "    buckety= map(x->x[2],bucket)\n",
    "    batch_x= fill(d.src.vocab.eos, length(bucketx), maximum(length.(bucketx)))\n",
    "    for i in 1:length(bucket)\n",
    "        batch_x[i, end-length(bucketx[i])+1:end] = bucketx[i]\n",
    "    end\n",
    "    batch_y= fill(d.tgt.vocab.eos, length(buckety),  maximum(length.(buckety) )+2)\n",
    "    for i in 1:length(bucket)\n",
    "        batch_y[i, 2:length(buckety[i])+1] = buckety[i]\n",
    "    end\n",
    "    \n",
    "    return (batch_x,batch_y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing MTData\n",
      "└ @ Main In[14]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MTData(TextReader(\"datasets/tr_to_en/tr.test\", Vocab(Dict(\"ağacından\"=>19226,\"ellisi\"=>3,\"komuta\"=>28565,\"adresini\"=>4,\"yüzeyi\"=>19227,\"paris'te\"=>5,\"kafamdaki\"=>28566,\"yüzeyinde\"=>19228,\"geçerlidir\"=>19229,\"kökten\"=>9729…), [\"<unk>\", \"<s>\", \"ellisi\", \"adresini\", \"paris'te\", \"uçaklardan\", \"buzulların\", \"hukukun\", \"kutuplaşma\", \"pedi\"  …  \"görünümü\", \"tahribatı\", \"yerdeyken\", \"kazandığında\", \"bilebilirsiniz\", \"planına\", \"köşedeki\", \"elimize\", \"gitmiştim\", \"muhafazakarlar\"], 1, 2, split)), TextReader(\"datasets/tr_to_en/en.test\", Vocab(Dict(\"middle-income\"=>4730,\"photosynthesis\"=>3,\"polarizing\"=>9462,\"henry\"=>4,\"abducted\"=>4731,\"whiz\"=>5,\"rises\"=>9463,\"hampshire\"=>14102,\"cost-benefit\"=>6,\"progression\"=>9464…), [\"<unk>\", \"<s>\", \"photosynthesis\", \"henry\", \"whiz\", \"cost-benefit\", \"gathered\", \"underground\", \"methods\", \"vis-a-vis\"  …  \"conquering\", \"backpack\", \"leans\", \"lap\", \"palestine\", \"convincing\", \"non-violent\", \"linguistics\", \"smuggled\", \"shorten\"], 1, 2, split)), 128, 9223372036854775807, false, 10, Array{Any,1}[[], [], [], [], [], [], [], [], [], []  …  [], [], [], [], [], [], [], [], [], []], arraybatch)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing MTData\"\n",
    "dtrn = MTData(tr_train, en_train)\n",
    "ddev = MTData(tr_dev, en_dev)\n",
    "\n",
    "dtst = MTData(tr_test, en_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: dtst not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: dtst not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[12]:2"
     ]
    }
   ],
   "source": [
    "'''\n",
    "x,y = first(dtst)\n",
    "@test length(collect(dtst)) == 48\n",
    "@test size.((x,y)) == ((128,10),(128,24))\n",
    "@test x[1,1] == tr_vocab.eos\n",
    "@test x[1,end] != tr_vocab.eos\n",
    "@test y[1,1] == en_vocab.eos\n",
    "@test y[1,2] != en_vocab.eos\n",
    "@test y[1,end] == en_vocab.eos\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Sequence to sequence model without attention\n",
    "\n",
    "In this part we will define a simple sequence to sequence encoder-decoder model for\n",
    "machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct S2S_v1\n",
    "    srcembed::Embed     # source language embedding\n",
    "    encoder::RNN        # encoder RNN (can be bidirectional)\n",
    "    tgtembed::Embed     # target language embedding\n",
    "    decoder::RNN        # decoder RNN\n",
    "    projection::Linear  # converts decoder output to vocab scores\n",
    "    dropout::Real       # dropout probability to prevent overfitting\n",
    "    srcvocab::Vocab     # source language vocabulary\n",
    "    tgtvocab::Vocab     # target language vocabulary\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@doc RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2S_v1 constructor\n",
    "\n",
    "Define the S2S_v1 constructor using your predefined layer types (Embed, Linear), and the\n",
    "Knet RNN type. Please review the RNN documentation using `@doc RNN`, paying attention to\n",
    "the following options in particular: `numLayers`, `bidirectional`, `dropout`, `dataType`,\n",
    "`usegpu`. The last two are important if you experiment with array types other than the\n",
    "default `KnetArray{Float32}`: make sure the RNNs use the same array type as the other\n",
    "layers. Note that if the encoder is bidirectional, its `numLayers` should be half of the\n",
    "decoder so that their hidden states match in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S_v1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function S2S_v1(hidden::Int,         # hidden size for both the encoder and decoder RNN\n",
    "                srcembsz::Int,       # embedding size for source language\n",
    "                tgtembsz::Int,       # embedding size for target language\n",
    "                srcvocab::Vocab,     # vocabulary for source language\n",
    "                tgtvocab::Vocab;     # vocabulary for target language\n",
    "                layers=1,            # number of layers\n",
    "                bidirectional=false, # whether encoder RNN is bidirectional\n",
    "                dropout=0)           # dropout probability\n",
    "    # Your code here\n",
    "    srcembed = Embed(length(srcvocab.i2w),srcembsz)\n",
    "    \n",
    "    encoder = RNN(size(srcembed),hidden,numLayers=layers,bidirectional=bidirectional ,dropout= dropout,usegpu=false)\n",
    "    \n",
    "    tgtembed = Embed(length(tgtvocab.i2w),tgtembsz)\n",
    "    if bidirectional\n",
    "        decoder = RNN(size(tgtembed),hidden,numLayers = 2*layers,dropout=dropout,usegpu=false)\n",
    "    else\n",
    "        decoder = RNN(size(tgtembed),hidden,numLayers = layers,dropout=dropout,usegpu=false)\n",
    "    end\n",
    "    projection = Linear(size(decoder),length(tgtvocab.i2w))\n",
    "    \n",
    "        \n",
    "    \n",
    "    S2S_v1(srcembed,encoder,tgtembed,decoder,projection,dropout,srcvocab,tgtvocab)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2S_v1 loss function\n",
    "\n",
    "Define the S2S_v1 loss function that takes `src`, a source language minibatch, and `tgt`,\n",
    "a target language minibatch and returns either a `(total_loss, num_words)` pair if\n",
    "`average=false`, or `(total_loss/num_words)` average if `average=true`.\n",
    "\n",
    "Assume that `src` and `tgt` are integer arrays of size `(B,Tx)` and `(B,Ty)` respectively,\n",
    "where `B` is the batch size, `Tx` is the length of the longest source sequence, `Ty` is\n",
    "the length of the longest target sequence. The `src` sequences only contain words, the\n",
    "`tgt` sequences surround the words with `eos` tokens at the start and end. This allows\n",
    "columns `tgt[:,1:end-1]` to be used as the decoder input and `tgt[:,2:end]` as the desired\n",
    "decoder output.\n",
    "\n",
    "Assume any shorter sentences in the batches have been padded with extra `eos` tokens on\n",
    "the left for `src` and on the right for `tgt`. Don't worry about masking `src` for the\n",
    "encoder, it doesn't have a significant effect on the loss. However do mask `tgt` before\n",
    "`nll`: you do not want the padding tokens to be counted in the loss calculation.\n",
    "\n",
    "Please review `@doc RNN`: in particular the `r.c` and `r.h` fields can be used to get/set\n",
    "the cell and hidden arrays of an RNN (note that `0` and `nothing` act as special values).\n",
    "\n",
    "RNNs take a dropout value at construction and apply dropout to the input of every layer if\n",
    "it is non-zero. You need to handle dropout for other layers in the loss function or in\n",
    "layer definitions as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "param(array; atype)\n",
       "param(dims...; init, atype)\n",
       "param0(dims...; atype)\n",
       "\\end{verbatim}\n",
       "The first form returns \\texttt{Param(atype(array))} where \\texttt{atype=identity} is the default.\n",
       "\n",
       "The second form Returns a randomly initialized \\texttt{Param(atype(init(dims...)))}. By default, \\texttt{init} is \\texttt{xavier} and \\texttt{atype} is \\texttt{KnetArray\\{Float32\\}} if \\texttt{gpu() >= 0}, \\texttt{Array\\{Float32\\}} otherwise. \n",
       "\n",
       "The third form \\texttt{param0} is an alias for \\texttt{param(dims...; init=zeros)}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "param(array; atype)\n",
       "param(dims...; init, atype)\n",
       "param0(dims...; atype)\n",
       "```\n",
       "\n",
       "The first form returns `Param(atype(array))` where `atype=identity` is the default.\n",
       "\n",
       "The second form Returns a randomly initialized `Param(atype(init(dims...)))`. By default, `init` is `xavier` and `atype` is `KnetArray{Float32}` if `gpu() >= 0`, `Array{Float32}` otherwise. \n",
       "\n",
       "The third form `param0` is an alias for `param(dims...; init=zeros)`.\n"
      ],
      "text/plain": [
       "\u001b[36m  param(array; atype)\u001b[39m\n",
       "\u001b[36m  param(dims...; init, atype)\u001b[39m\n",
       "\u001b[36m  param0(dims...; atype)\u001b[39m\n",
       "\n",
       "  The first form returns \u001b[36mParam(atype(array))\u001b[39m where \u001b[36matype=identity\u001b[39m is the\n",
       "  default.\n",
       "\n",
       "  The second form Returns a randomly initialized \u001b[36mParam(atype(init(dims...)))\u001b[39m.\n",
       "  By default, \u001b[36minit\u001b[39m is \u001b[36mxavier\u001b[39m and \u001b[36matype\u001b[39m is \u001b[36mKnetArray{Float32}\u001b[39m if \u001b[36mgpu() >= 0\u001b[39m,\n",
       "  \u001b[36mArray{Float32}\u001b[39m otherwise. \n",
       "\n",
       "  The third form \u001b[36mparam0\u001b[39m is an alias for \u001b[36mparam(dims...; init=zeros)\u001b[39m."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "\\end{verbatim}\n",
       "\\texttt{RNN} returns a callable RNN object \\texttt{rnn}. Given a minibatch of sequences \\texttt{x}, \\texttt{rnn(x)} returns \\texttt{y}, the hidden states of the final layer for each time step. \\texttt{rnn.h} and \\texttt{rnn.c} fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of \\texttt{y} always contains the final hidden state of the last layer, equivalent to \\texttt{rnn.h} for a single layer network.\n",
       "\n",
       "\\textbf{Dimensions:} The input \\texttt{x} can be 1, 2, or 3 dimensional and \\texttt{y} will have the same number of dimensions as \\texttt{x}. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D \\texttt{x} represents a single instance for a single time step, a 2-D \\texttt{x} represents a single minibatch for a single time step, and a 3-D \\texttt{x} represents a sequence of identically sized minibatches for multiple time steps. The output \\texttt{y} gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields \\texttt{rnn.h} and \\texttt{rnn.c} represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "\\textbf{batchSizes:} If \\texttt{batchSizes=nothing} (default), all sequences in a minibatch are assumed to be the same length. If \\texttt{batchSizes} is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case \\texttt{x} will typically be 2-D with the second dimension representing variable size batches for time steps. If \\texttt{batchSizes} is used, \\texttt{sum(batchSizes)} should equal \\texttt{length(x) ÷ size(x,1)}. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "\\textbf{Hidden states:} The hidden and cell states are kept in \\texttt{rnn.h} and \\texttt{rnn.c} fields (the cell state is only used by LSTM). They can be initialized during construction using the \\texttt{h} and \\texttt{c} keyword arguments, or modified later by direct assignment. Valid values are \\texttt{nothing} (default), \\texttt{0}, or an array of the right type and size possibly wrapped in a \\texttt{Param}. If the value is \\texttt{nothing} the initial state is assumed to be zero and the final state is discarded keeping the value \\texttt{nothing}. If the value is \\texttt{0} the initial state is assumed to be zero and \\texttt{0} is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in \\texttt{Result} types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. \\texttt{rnn.h = value(rnn.h)} to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the \\href{https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb}{CharLM Tutorial} for an example.\n",
       "\n",
       "\\textbf{Keyword arguments for RNN:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{h=nothing}: Initial hidden state.\n",
       "\n",
       "\n",
       "\\item \\texttt{c=nothing}: Initial cell state.\n",
       "\n",
       "\n",
       "\\item \\texttt{rnnType=:lstm} Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "\n",
       "\\item \\texttt{numLayers=1}: Number of RNN layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{bidirectional=false}: Create a bidirectional RNN if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dropout=0}: Dropout probability. Applied to input and between layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{skipInput=false}: Do not multiply the input with a matrix if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dataType=Float32}: Data type to use for weights.\n",
       "\n",
       "\n",
       "\\item \\texttt{algo=0}: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "\n",
       "\\item \\texttt{seed=0}: Random number seed for dropout. Uses \\texttt{time()} if 0.\n",
       "\n",
       "\n",
       "\\item \\texttt{winit=xavier}: Weight initialization method for matrices.\n",
       "\n",
       "\n",
       "\\item \\texttt{binit=zeros}: Weight initialization method for bias vectors.\n",
       "\n",
       "\n",
       "\\item \\texttt{finit=ones}: Weight initialization method for the bias of forget gates.\n",
       "\n",
       "\n",
       "\\item \\texttt{usegpu=(gpu()>=0)}: GPU used by default if one exists.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Formulas:} RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "\\texttt{:relu} and \\texttt{:tanh}: Single gate RNN with activation function f:\n",
       "\n",
       "\\begin{verbatim}\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "\\end{verbatim}\n",
       "\\texttt{:gru}: Gated recurrent unit:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "\\end{verbatim}\n",
       "\\texttt{:lstm}: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "```\n",
       "\n",
       "`RNN` returns a callable RNN object `rnn`. Given a minibatch of sequences `x`, `rnn(x)` returns `y`, the hidden states of the final layer for each time step. `rnn.h` and `rnn.c` fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of `y` always contains the final hidden state of the last layer, equivalent to `rnn.h` for a single layer network.\n",
       "\n",
       "**Dimensions:** The input `x` can be 1, 2, or 3 dimensional and `y` will have the same number of dimensions as `x`. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D `x` represents a single instance for a single time step, a 2-D `x` represents a single minibatch for a single time step, and a 3-D `x` represents a sequence of identically sized minibatches for multiple time steps. The output `y` gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields `rnn.h` and `rnn.c` represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "**batchSizes:** If `batchSizes=nothing` (default), all sequences in a minibatch are assumed to be the same length. If `batchSizes` is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case `x` will typically be 2-D with the second dimension representing variable size batches for time steps. If `batchSizes` is used, `sum(batchSizes)` should equal `length(x) ÷ size(x,1)`. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "**Hidden states:** The hidden and cell states are kept in `rnn.h` and `rnn.c` fields (the cell state is only used by LSTM). They can be initialized during construction using the `h` and `c` keyword arguments, or modified later by direct assignment. Valid values are `nothing` (default), `0`, or an array of the right type and size possibly wrapped in a `Param`. If the value is `nothing` the initial state is assumed to be zero and the final state is discarded keeping the value `nothing`. If the value is `0` the initial state is assumed to be zero and `0` is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in `Result` types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. `rnn.h = value(rnn.h)` to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the [CharLM Tutorial](https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb) for an example.\n",
       "\n",
       "**Keyword arguments for RNN:**\n",
       "\n",
       "  * `h=nothing`: Initial hidden state.\n",
       "  * `c=nothing`: Initial cell state.\n",
       "  * `rnnType=:lstm` Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "  * `numLayers=1`: Number of RNN layers.\n",
       "  * `bidirectional=false`: Create a bidirectional RNN if `true`.\n",
       "  * `dropout=0`: Dropout probability. Applied to input and between layers.\n",
       "  * `skipInput=false`: Do not multiply the input with a matrix if `true`.\n",
       "  * `dataType=Float32`: Data type to use for weights.\n",
       "  * `algo=0`: Algorithm to use, see CUDNN docs for details.\n",
       "  * `seed=0`: Random number seed for dropout. Uses `time()` if 0.\n",
       "  * `winit=xavier`: Weight initialization method for matrices.\n",
       "  * `binit=zeros`: Weight initialization method for bias vectors.\n",
       "  * `finit=ones`: Weight initialization method for the bias of forget gates.\n",
       "  * `usegpu=(gpu()>=0)`: GPU used by default if one exists.\n",
       "\n",
       "**Formulas:** RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "`:relu` and `:tanh`: Single gate RNN with activation function f:\n",
       "\n",
       "```\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "```\n",
       "\n",
       "`:gru`: Gated recurrent unit:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "```\n",
       "\n",
       "`:lstm`: Long short term memory unit with no peephole connections:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  rnn = RNN(inputSize, hiddenSize; opts...)\u001b[39m\n",
       "\u001b[36m  rnn(x; batchSizes) => y\u001b[39m\n",
       "\u001b[36m  rnn.h, rnn.c  # hidden and cell states\u001b[39m\n",
       "\n",
       "  \u001b[36mRNN\u001b[39m returns a callable RNN object \u001b[36mrnn\u001b[39m. Given a minibatch of sequences \u001b[36mx\u001b[39m,\n",
       "  \u001b[36mrnn(x)\u001b[39m returns \u001b[36my\u001b[39m, the hidden states of the final layer for each time step.\n",
       "  \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m fields can be used to set the initial hidden states and read\n",
       "  the final hidden states of all layers. Note that the final time step of \u001b[36my\u001b[39m\n",
       "  always contains the final hidden state of the last layer, equivalent to\n",
       "  \u001b[36mrnn.h\u001b[39m for a single layer network.\n",
       "\n",
       "  \u001b[1mDimensions:\u001b[22m The input \u001b[36mx\u001b[39m can be 1, 2, or 3 dimensional and \u001b[36my\u001b[39m will have the\n",
       "  same number of dimensions as \u001b[36mx\u001b[39m. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T])\n",
       "  where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is\n",
       "  for bidirectional RNNs. By default a 1-D \u001b[36mx\u001b[39m represents a single instance for\n",
       "  a single time step, a 2-D \u001b[36mx\u001b[39m represents a single minibatch for a single time\n",
       "  step, and a 3-D \u001b[36mx\u001b[39m represents a sequence of identically sized minibatches for\n",
       "  multiple time steps. The output \u001b[36my\u001b[39m gives the hidden state (of the final layer\n",
       "  for multi-layer RNNs) for each time step. The fields \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m\n",
       "  represent the hidden states of all layers in a single time step and have\n",
       "  size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "  \u001b[1mbatchSizes:\u001b[22m If \u001b[36mbatchSizes=nothing\u001b[39m (default), all sequences in a minibatch\n",
       "  are assumed to be the same length. If \u001b[36mbatchSizes\u001b[39m is an array of\n",
       "  (non-increasing) integers, it gives us the batch size for each time step\n",
       "  (allowing different sequences in the minibatch to have different lengths).\n",
       "  In this case \u001b[36mx\u001b[39m will typically be 2-D with the second dimension representing\n",
       "  variable size batches for time steps. If \u001b[36mbatchSizes\u001b[39m is used, \u001b[36msum(batchSizes)\u001b[39m\n",
       "  should equal \u001b[36mlength(x) ÷ size(x,1)\u001b[39m. When the batch size is different in\n",
       "  every time step, hidden states will have size (H,B,L/2L) where B is always\n",
       "  the size of the first (largest) minibatch.\n",
       "\n",
       "  \u001b[1mHidden states:\u001b[22m The hidden and cell states are kept in \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m fields\n",
       "  (the cell state is only used by LSTM). They can be initialized during\n",
       "  construction using the \u001b[36mh\u001b[39m and \u001b[36mc\u001b[39m keyword arguments, or modified later by\n",
       "  direct assignment. Valid values are \u001b[36mnothing\u001b[39m (default), \u001b[36m0\u001b[39m, or an array of the\n",
       "  right type and size possibly wrapped in a \u001b[36mParam\u001b[39m. If the value is \u001b[36mnothing\u001b[39m the\n",
       "  initial state is assumed to be zero and the final state is discarded keeping\n",
       "  the value \u001b[36mnothing\u001b[39m. If the value is \u001b[36m0\u001b[39m the initial state is assumed to be zero\n",
       "  and \u001b[36m0\u001b[39m is replaced by the final state on return. If the value is a valid\n",
       "  state, it is used as the initial state and is replaced by the final state on\n",
       "  return.\n",
       "\n",
       "  In a differentiation context the returned final hidden states will be\n",
       "  wrapped in \u001b[36mResult\u001b[39m types. This is necessary if the same RNN object is to be\n",
       "  called multiple times in a single iteration. Between iterations (i.e. after\n",
       "  diff/update) the hidden states need to be unboxed with e.g. \u001b[36mrnn.h =\n",
       "  value(rnn.h)\u001b[39m to prevent spurious dependencies. This happens automatically\n",
       "  during the backward pass for GPU RNNs but needs to be done manually for CPU\n",
       "  RNNs. See the CharLM Tutorial\n",
       "  (https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb)\n",
       "  for an example.\n",
       "\n",
       "  \u001b[1mKeyword arguments for RNN:\u001b[22m\n",
       "\n",
       "    •    \u001b[36mh=nothing\u001b[39m: Initial hidden state.\n",
       "\n",
       "    •    \u001b[36mc=nothing\u001b[39m: Initial cell state.\n",
       "\n",
       "    •    \u001b[36mrnnType=:lstm\u001b[39m Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "    •    \u001b[36mnumLayers=1\u001b[39m: Number of RNN layers.\n",
       "\n",
       "    •    \u001b[36mbidirectional=false\u001b[39m: Create a bidirectional RNN if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdropout=0\u001b[39m: Dropout probability. Applied to input and between\n",
       "        layers.\n",
       "\n",
       "    •    \u001b[36mskipInput=false\u001b[39m: Do not multiply the input with a matrix if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdataType=Float32\u001b[39m: Data type to use for weights.\n",
       "\n",
       "    •    \u001b[36malgo=0\u001b[39m: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "    •    \u001b[36mseed=0\u001b[39m: Random number seed for dropout. Uses \u001b[36mtime()\u001b[39m if 0.\n",
       "\n",
       "    •    \u001b[36mwinit=xavier\u001b[39m: Weight initialization method for matrices.\n",
       "\n",
       "    •    \u001b[36mbinit=zeros\u001b[39m: Weight initialization method for bias vectors.\n",
       "\n",
       "    •    \u001b[36mfinit=ones\u001b[39m: Weight initialization method for the bias of forget\n",
       "        gates.\n",
       "\n",
       "    •    \u001b[36musegpu=(gpu()>=0)\u001b[39m: GPU used by default if one exists.\n",
       "\n",
       "  \u001b[1mFormulas:\u001b[22m RNNs compute the output h[t] for a given iteration from the\n",
       "  recurrent input h[t-1] and the previous layer input x[t] given matrices W, R\n",
       "  and biases bW, bR from the following equations:\n",
       "\n",
       "  \u001b[36m:relu\u001b[39m and \u001b[36m:tanh\u001b[39m: Single gate RNN with activation function f:\n",
       "\n",
       "\u001b[36m  h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\u001b[39m\n",
       "\n",
       "  \u001b[36m:gru\u001b[39m: Gated recurrent unit:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\u001b[39m\n",
       "\u001b[36m  h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\u001b[39m\n",
       "\n",
       "  \u001b[36m:lstm\u001b[39m: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\u001b[39m\n",
       "\u001b[36m  o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\u001b[39m\n",
       "\u001b[36m  c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\u001b[39m\n",
       "\u001b[36m  h[t] = o[t] .* tanh(c[t])\u001b[39m"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S_v1)(src, tgt; average=true)\n",
    "    # Your code here\n",
    "    embout = s.embed(src)\n",
    "    b,tx = size(src)\n",
    "    @assert size(embout)== (s.srcembsz,b,tx)\n",
    "    rnnencoder=s.encoder(embout)\n",
    "    \n",
    "    embouttgt = s.emb(tgt[:,1:end-1])\n",
    "    rnndecoder = s.decoder(embouttgt;h= rnnencoder.h,c=rnnencoder.c)\n",
    "    rnndecoder\n",
    "    b,ty = size(tgt)\n",
    "    @assert size(rnndecoder)==(rnndecoder.h,b,ty)\n",
    "    \n",
    "    \n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing S2S_v1\n",
      "└ @ Main In[24]:1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TypeError: in Type{...} expression, expected UnionAll, got typeof(Knet.CuArray)",
     "output_type": "error",
     "traceback": [
      "TypeError: in Type{...} expression, expected UnionAll, got typeof(Knet.CuArray)",
      "",
      "Stacktrace:",
      " [1] KnetPtrCu(::Int64) at /home/burak/.julia/packages/Knet/8zOGv/src/cuarray.jl:95",
      " [2] Knet.KnetPtr(::Int64) at /home/burak/.julia/packages/Knet/8zOGv/src/kptr.jl:107",
      " [3] KnetArray{Float32,N} where N(::UndefInitializer, ::Tuple{Int64,Int64}) at /home/burak/.julia/packages/Knet/8zOGv/src/karray.jl:82",
      " [4] KnetArray{Float32,2}(::Array{Float64,2}) at /home/burak/.julia/packages/Knet/8zOGv/src/karray.jl:95",
      " [5] KnetArray{Float32,N} where N(::Array{Float64,2}) at /home/burak/.julia/packages/Knet/8zOGv/src/karray.jl:94",
      " [6] #param#684(::Function, ::Type, ::Function, ::Int64, ::Vararg{Int64,N} where N) at /home/burak/.julia/packages/Knet/8zOGv/src/train.jl:102",
      " [7] param(::Int64, ::Int64) at /home/burak/.julia/packages/Knet/8zOGv/src/train.jl:102",
      " [8] Embed(::Int64, ::Int64) at ./In[4]:6",
      " [9] #S2S_v1#20(::Int64, ::Bool, ::Float64, ::Type, ::Int64, ::Int64, ::Int64, ::Vocab, ::Vocab) at ./In[21]:10",
      " [10] (::getfield(Core, Symbol(\"#kw#Type\")))(::NamedTuple{(:layers, :bidirectional, :dropout),Tuple{Int64,Bool,Float64}}, ::Type{S2S_v1}, ::Int64, ::Int64, ::Int64, ::Vocab, ::Vocab) at ./none:0",
      " [11] top-level scope at In[24]:2"
     ]
    }
   ],
   "source": [
    "@info \"Testing S2S_v1\"\n",
    "#Knet.seed!(1)\n",
    "model = S2S_v1(512, 512, 512, tr_vocab, en_vocab; layers=2, bidirectional=true, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing S2S_v1\n",
      "└ @ Main In[18]:1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TypeError: in Type{...} expression, expected UnionAll, got typeof(Knet.CuArray)",
     "output_type": "error",
     "traceback": [
      "TypeError: in Type{...} expression, expected UnionAll, got typeof(Knet.CuArray)",
      "",
      "Stacktrace:",
      " [1] KnetPtrCu(::Int64) at /home/burak/.julia/packages/Knet/8zOGv/src/cuarray.jl:95",
      " [2] Knet.KnetPtr(::Int64) at /home/burak/.julia/packages/Knet/8zOGv/src/kptr.jl:107",
      " [3] KnetArray{Float32,N} where N(::UndefInitializer, ::Tuple{Int64,Int64}) at /home/burak/.julia/packages/Knet/8zOGv/src/karray.jl:82",
      " [4] KnetArray{Float32,2}(::Array{Float64,2}) at /home/burak/.julia/packages/Knet/8zOGv/src/karray.jl:95",
      " [5] KnetArray{Float32,N} where N(::Array{Float64,2}) at /home/burak/.julia/packages/Knet/8zOGv/src/karray.jl:94",
      " [6] #param#684(::Function, ::Type, ::Function, ::Int64, ::Vararg{Int64,N} where N) at /home/burak/.julia/packages/Knet/8zOGv/src/train.jl:102",
      " [7] param(::Int64, ::Int64) at /home/burak/.julia/packages/Knet/8zOGv/src/train.jl:102",
      " [8] Embed(::Int64, ::Int64) at ./In[4]:6",
      " [9] #S2S_v1#18(::Int64, ::Bool, ::Float64, ::Type, ::Int64, ::Int64, ::Int64, ::Vocab, ::Vocab) at ./In[16]:10",
      " [10] (::getfield(Core, Symbol(\"#kw#Type\")))(::NamedTuple{(:layers, :bidirectional, :dropout),Tuple{Int64,Bool,Float64}}, ::Type{S2S_v1}, ::Int64, ::Int64, ::Int64, ::Vocab, ::Vocab) at ./none:0",
      " [11] top-level scope at In[18]:3"
     ]
    }
   ],
   "source": [
    "(x,y) = first(dtst)\n",
    "# Your loss can be slightly different due to different ordering of words in the vocabulary.\n",
    "# The reference vocabulary starts with eos, unk, followed by words in decreasing frequency.\n",
    "@test model(x,y; average=false) == (14097.471f0, 1432)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss for a whole dataset\n",
    "\n",
    "Define a `loss(model, data)` which returns a `(Σloss, Nloss)` pair if `average=false` and\n",
    "a `Σloss/Nloss` average if `average=true` for a whole dataset. Assume that `data` is an\n",
    "iterator of `(x,y)` pairs such as `MTData` and `model(x,y;average)` is a model like\n",
    "`S2S_v1` that computes loss on a single `(x,y)` pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(model, data; average=true)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Testing loss\"\n",
    "@test loss(model, dtst, average=false) == (1.0429117f6, 105937)\n",
    "# Your loss can be slightly different due to different ordering of words in the vocabulary.\n",
    "# The reference vocabulary starts with eos, unk, followed by words in decreasing frequency.\n",
    "# Also, because we do not mask src, different batch sizes may lead to slightly different\n",
    "# losses. The test above gives (1.0429178f6, 105937) with batchsize==1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SGD_v1\n",
    "\n",
    "The following function can be used to train our model. `trn` is the training data, `dev`\n",
    "is used to determine the best model, `tst...` can be zero or more small test datasets for\n",
    "loss reporting. It returns the model that does best on `dev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train!(model, trn, dev, tst...)\n",
    "    bestmodel, bestloss = deepcopy(model), loss(model, dev)\n",
    "    progress!(adam(model, trn), steps=100) do y\n",
    "        losses = [ loss(model, d) for d in (dev,tst...) ]\n",
    "        if losses[1] < bestloss\n",
    "            bestmodel, bestloss = deepcopy(model), losses[1]\n",
    "        end\n",
    "        return (losses...,)\n",
    "    end\n",
    "    return bestmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to get under 3.40 dev loss with the following settings in 10\n",
    "epochs. The training speed on a V100 is about 3 mins/epoch or 40K words/sec, K80 is about\n",
    "6 times slower. Using settings closer to the Luong paper (per-sentence loss rather than\n",
    "per-word loss, SGD with lr=1, gclip=1 instead of Adam), you can get to 3.17 dev loss in\n",
    "about 25 epochs. Using dropout and shuffling batches before each epoch significantly\n",
    "improve the dev loss. You can play around with hyperparameters but I doubt results will\n",
    "get much better without attention. To verify your training, here is the dev loss I\n",
    "observed at the beginning of each epoch in one training session:\n",
    "`[9.83, 4.60, 3.98, 3.69, 3.52, 3.41, 3.35, 3.32, 3.30, 3.31, 3.33]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Training S2S_v1\"\n",
    "epochs = 10\n",
    "ctrn = collect(dtrn)\n",
    "trnx10 = collect(flatten(shuffle!(ctrn) for i in 1:epochs))\n",
    "trn20 = ctrn[1:20]\n",
    "dev38 = collect(ddev)\n",
    "# Uncomment this to train the model (This takes about 30 mins on a V100):\n",
    "# model = train!(model, trnx10, dev38, trn20)\n",
    "# Uncomment this to save the model:\n",
    "# Knet.save(\"s2s_v1.jld2\",\"model\",model)\n",
    "# Uncomment this to load the model:\n",
    "# model = Knet.load(\"s2s_v1.jld2\",\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating translations\n",
    "\n",
    "With a single argument, a `S2S_v1` object should take it as a batch of source sentences\n",
    "and generate translations for them. After passing `src` through the encoder and copying\n",
    "its hidden states to the decoder, the decoder is run starting with an initial input of all\n",
    "`eos` tokens. Highest scoring tokens are appended to the output and used as input for the\n",
    "subsequent decoder steps.  The decoder should stop generating when all sequences in the\n",
    "batch have generated `eos` or when `stopfactor * size(src,2)` decoder steps are reached. A\n",
    "correctly shaped target language batch should be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S_v1)(src::Matrix{Int}; stopfactor = 3)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to convert int arrays to sentence strings\n",
    "function int2str(y,vocab)\n",
    "    y = vec(y)\n",
    "    ysos = findnext(w->!isequal(w,vocab.eos), y, 1)\n",
    "    ysos == nothing && return \"\"\n",
    "    yeos = something(findnext(isequal(vocab.eos), y, ysos), 1+length(y))\n",
    "    join(vocab.i2w[y[ysos:yeos-1]], \" \")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Generating some translations\"\n",
    "d = MTData(tr_dev, en_dev, batchsize=1) |> collect\n",
    "(src,tgt) = rand(d)\n",
    "out = model(src)\n",
    "println(\"SRC: \", int2str(src,model.srcvocab))\n",
    "println(\"REF: \", int2str(tgt,model.tgtvocab))\n",
    "println(\"OUT: \", int2str(out,model.tgtvocab))\n",
    "# Here is a sample output:\n",
    "# SRC: çin'e 15 şubat 2006'da ulaştım .\n",
    "# REF: i made it to china on february 15 , 2006 .\n",
    "# OUT: i got to china , china , at the last 15 years ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating BLEU\n",
    "\n",
    "BLEU is the most commonly used metric to measure translation quality. The following should\n",
    "take a model and some data, generate translations and calculate BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function bleu(s2s,d::MTData)\n",
    "    d = MTData(d.src,d.tgt,batchsize=1)\n",
    "    reffile = d.tgt.file\n",
    "    hypfile,hyp = mktemp()\n",
    "    for (x,y) in progress(collect(d))\n",
    "        g = s2s(x)\n",
    "        for i in 1:size(y,1)\n",
    "            println(hyp, int2str(g[i,:], d.tgt.vocab))\n",
    "        end\n",
    "    end\n",
    "    close(hyp)\n",
    "    isfile(\"multi-bleu.perl\") || download(\"https://github.com/moses-smt/mosesdecoder/raw/master/scripts/generic/multi-bleu.perl\", \"multi-bleu.perl\")\n",
    "    run(pipeline(`cat $hypfile`,`perl multi-bleu.perl $reffile`))\n",
    "    return hypfile\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating dev BLEU takes about 45 secs on a V100. We get about 8.0 BLEU which is pretty\n",
    "low. As can be seen from the sample translations a loss of ~3+ (perplexity ~20+) or a BLEU\n",
    "of ~8 is not sufficient to generate meaningful translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Calculating BLEU\"\n",
    "bleu(model, ddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the quality of translations we can use more training data, different training\n",
    "and model parameters, or preprocess the input/output: e.g. splitting Turkish words to make\n",
    "suffixes look more like English function words may help. Other architectures,\n",
    "e.g. attention and transformer, perform significantly better than this simple S2S model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
